{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= POS-aware Cleaning Pipeline (FP, adjusted) =========\n",
    "import os, re, glob, json, pickle, random\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "# ----- Ensure NLTK resources -----\n",
    "for pkg in (\"stopwords\", \"punkt\", \"wordnet\"):\n",
    "    try:\n",
    "        nltk.data.find(f\"corpora/{pkg}\" if pkg != \"punkt\" else \"tokenizers/punkt\")\n",
    "    except LookupError:\n",
    "        nltk.download(pkg)\n",
    "\n",
    "try:\n",
    "    nltk.data.find(\"taggers/averaged_perceptron_tagger_eng\")\n",
    "except LookupError:\n",
    "    try:\n",
    "        nltk.download(\"averaged_perceptron_tagger_eng\")\n",
    "    except:\n",
    "        nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "from nltk.corpus import stopwords, wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag_sents\n",
    "\n",
    "# ----- Config -----\n",
    "INPUT_ROOTS = [\n",
    "    os.path.join(\"..\", \"data\", \"raw\")\n",
    "]\n",
    "OUTPUT_DIR  = os.path.join(\"..\", \"data\", \"final\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "GLOB_PATTERNS = {\n",
    "    \"Fortnite_Ninja\":    \"Fortnite*Ninja*Comments*Analysis.parquet\",\n",
    "    \"Fortnite_SypherPK\": \"Fortnite*Sypher*Comments*Analysis.parquet\",\n",
    "    \"Fortnite_NickEh30\": \"Fortnite*Nick*Eh*30*Comments*Analysis.parquet\",\n",
    "    \"Apex Legends\":      \"Apex*Legends*Comments*Analysis.parquet\",\n",
    "    \"Rocket League\":     \"Rocket*League*Comments*Analysis.parquet\",\n",
    "    \"DOTA 2\":            \"DOTA*2*Comments*Analysis.parquet\",\n",
    "    \"Valorant\":          \"Valorant*Comments*Analysis.parquet\",\n",
    "}\n",
    "\n",
    "# Phrase thresholds (tightened for big corpus)\n",
    "BIGRAM_MIN_COUNT = 10\n",
    "PHRASE_THRESHOLD = 8.0\n",
    "\n",
    "# Row min tokens\n",
    "MIN_TOKENS_ROW   = 5\n",
    "\n",
    "# Dictionary pruning\n",
    "NO_BELOW = 5\n",
    "NO_ABOVE = 0.50\n",
    "KEEP_N   = 100_000\n",
    "\n",
    "# Keep or drop franchise tokens in the GLOBAL model\n",
    "KEEP_FRANCHISE_TOKENS = False  # set True if you want topics to explicitly include franchise names\n",
    "\n",
    "# ----- Stopwords -----\n",
    "NLTK_STOP = set(stopwords.words(\"english\"))\n",
    "CUSTOM_STOP = {\n",
    "    # ... [same as before, omitted for brevity]\n",
    "}\n",
    "if not KEEP_FRANCHISE_TOKENS:\n",
    "    CUSTOM_STOP |= {'fortnite','apex','valorant','rocket_league','dota'}\n",
    "\n",
    "STOP_WORDS = NLTK_STOP.union(CUSTOM_STOP)\n",
    "\n",
    "# ----- Regex/helpers -----\n",
    "URL_RE   = re.compile(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\")\n",
    "HTML_RE  = re.compile(r\"<.*?>\")\n",
    "PUNC_RE  = re.compile(r\"[^\\w\\s]\")\n",
    "DIGIT_RE = re.compile(r\"\\d+\")\n",
    "WS_RE    = re.compile(r\"\\s+\")\n",
    "LEMM     = WordNetLemmatizer()\n",
    "\n",
    "LEMMA_FIX = {\n",
    "    'of_thief': 'sea_of_thieves',\n",
    "    'sea_of_thief': 'sea_of_thieves',\n",
    "    'sea_of_thief_sea': 'sea_of_thieves',\n",
    "}\n",
    "\n",
    "BAD_PHRASE = re.compile(r'^[a-z]_[a-z]$')\n",
    "\n",
    "def _wn_pos(tag: str):\n",
    "    if not tag: return wn.NOUN\n",
    "    t = tag[0]\n",
    "    return wn.ADJ if t == 'J' else wn.VERB if t == 'V' else wn.NOUN if t == 'N' else wn.ADV if t == 'R' else wn.NOUN\n",
    "\n",
    "def normalize(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = URL_RE.sub(\" \", text)\n",
    "    text = HTML_RE.sub(\" \", text)\n",
    "    text = PUNC_RE.sub(\" \", text)\n",
    "    text = DIGIT_RE.sub(\" \", text)\n",
    "    text = WS_RE.sub(\" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def tokenize_simple(text: str):\n",
    "    return text.split()\n",
    "\n",
    "def pos_lemmatize(tokens):\n",
    "    if not tokens:\n",
    "        return []\n",
    "    tagged = list(pos_tag_sents([tokens]))[0]\n",
    "    return [LEMM.lemmatize(w, _wn_pos(tag)) for (w, tag) in tagged]\n",
    "\n",
    "def resolve_path(pattern, roots):\n",
    "    for root in roots:\n",
    "        matches = glob.glob(os.path.join(root, pattern))\n",
    "        if matches:\n",
    "            matches.sort(key=lambda p: os.path.getmtime(p), reverse=True)\n",
    "            return matches[0]\n",
    "    return None\n",
    "\n",
    "# ---------- Load, normalize, POS-lemma ----------\n",
    "raw_dfs, missing = [], []\n",
    "for label, pat in GLOB_PATTERNS.items():\n",
    "    fpath = resolve_path(pat, INPUT_ROOTS)\n",
    "    if not fpath:\n",
    "        print(f\"âš ï¸ No match for {label} with pattern {pat} in {INPUT_ROOTS}\")\n",
    "        missing.append(label)\n",
    "        continue\n",
    "\n",
    "    df = pd.read_parquet(fpath)\n",
    "    if not {'author','text'}.issubset(df.columns):\n",
    "        print(f\"âš ï¸ Required columns missing in {os.path.basename(fpath)} â€” skipping.\")\n",
    "        continue\n",
    "\n",
    "    df = df.dropna(subset=['author','text']).copy()\n",
    "    df['__norm'] = df['text'].map(lambda t: normalize(t) if isinstance(t,str) else \"\")\n",
    "    df['__raw_tokens'] = df['__norm'].map(tokenize_simple)\n",
    "    df['raw_tokens'] = df['__raw_tokens'].map(pos_lemmatize)\n",
    "    df['creator_or_game'] = label\n",
    "\n",
    "    raw_dfs.append(df[['author','text','raw_tokens','creator_or_game']])\n",
    "    print(f\"âœ… {label}: {len(df)} rows â€” {os.path.basename(fpath)}\")\n",
    "\n",
    "if not raw_dfs:\n",
    "    raise SystemExit(\"No valid FP inputs loaded.\")\n",
    "\n",
    "fp = pd.concat(raw_dfs, ignore_index=True)\n",
    "print(\"ðŸ“Š Per-source counts:\", fp['creator_or_game'].value_counts().to_dict())\n",
    "\n",
    "# ---------- Train phrases on raw tokens (pre-stopwords) ----------\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "\n",
    "bigram  = Phrases(fp['raw_tokens'], min_count=BIGRAM_MIN_COUNT, threshold=PHRASE_THRESHOLD)\n",
    "trigram = Phrases(bigram[fp['raw_tokens']], threshold=PHRASE_THRESHOLD)\n",
    "bigram_phraser  = Phraser(bigram)\n",
    "trigram_phraser = Phraser(trigram)\n",
    "\n",
    "def apply_phrases_then_filter(toks):\n",
    "    phr = trigram_phraser[bigram_phraser[toks]]\n",
    "    phr = [w for w in phr if not BAD_PHRASE.match(w)]\n",
    "    phr = [LEMMA_FIX.get(w, w) for w in phr]\n",
    "    return [w for w in phr if w not in STOP_WORDS and len(w) > 2]\n",
    "\n",
    "fp['tokens'] = fp['raw_tokens'].apply(apply_phrases_then_filter)\n",
    "\n",
    "# ---------- Row-level min-length filter ----------\n",
    "initial = len(fp)\n",
    "fp = fp[fp['tokens'].str.len() >= MIN_TOKENS_ROW]\n",
    "print(f\"âœ… Removed {initial - len(fp)} short comments (<{MIN_TOKENS_ROW} tokens).\")\n",
    "\n",
    "# ---------- Peek tokens ----------\n",
    "all_tokens = [w for toks in fp['tokens'] for w in toks]\n",
    "print(\"ðŸ”¹ Top 50 tokens:\", Counter(all_tokens).most_common(50))\n",
    "\n",
    "# ---------- Save cleaned ----------\n",
    "clean_path = os.path.join(OUTPUT_DIR, \"Filtered_Combined_FP_Cleaned.parquet\")\n",
    "fp.to_parquet(clean_path, index=False)\n",
    "print(f\"ðŸ’¾ Saved cleaned data -> {clean_path}\")\n",
    "\n",
    "# ---------- Dictionary / Corpus (with pruning) ----------\n",
    "from gensim.corpora import Dictionary\n",
    "dictionary = Dictionary(fp['tokens'])\n",
    "dictionary.filter_extremes(no_below=NO_BELOW, no_above=NO_ABOVE, keep_n=KEEP_N)\n",
    "corpus = [dictionary.doc2bow(t) for t in fp['tokens']]\n",
    "print(f\"ðŸ“š Dictionary: {len(dictionary)} tokens | Corpus docs: {len(corpus)}\")\n",
    "\n",
    "dict_path = os.path.join(OUTPUT_DIR, \"lda_dictionary_FP.dict\")\n",
    "dictionary.save(dict_path)\n",
    "\n",
    "# Save phrasers & corpus for reuse\n",
    "bigram_phraser.save(os.path.join(OUTPUT_DIR, \"bigram_FP.pkl\"))\n",
    "trigram_phraser.save(os.path.join(OUTPUT_DIR, \"trigram_FP.pkl\"))\n",
    "with open(os.path.join(OUTPUT_DIR, \"lda_corpus_FP.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(corpus, f)\n",
    "\n",
    "# Save basic metadata\n",
    "with open(os.path.join(OUTPUT_DIR, \"cleaning_meta_FP.json\"), \"w\") as f:\n",
    "    json.dump({\n",
    "        \"no_below\": NO_BELOW,\n",
    "        \"no_above\": NO_ABOVE,\n",
    "        \"keep_n\": KEEP_N,\n",
    "        \"bigram_min_count\": BIGRAM_MIN_COUNT,\n",
    "        \"phrase_threshold\": PHRASE_THRESHOLD,\n",
    "        \"min_tokens_row\": MIN_TOKENS_ROW,\n",
    "        \"keep_franchise_tokens\": KEEP_FRANCHISE_TOKENS,\n",
    "        \"stopwords_sizes\": {\"nltk\": len(NLTK_STOP), \"custom\": len(CUSTOM_STOP)}\n",
    "    }, f, indent=2)\n",
    "\n",
    "# ---------- Stratified 90/10 split by source (creator/game) ----------\n",
    "rng_state = 11\n",
    "by_src = defaultdict(list)\n",
    "for i, src in enumerate(fp['creator_or_game']):\n",
    "    by_src[src].append(i)\n",
    "\n",
    "hold_idx = set()\n",
    "for src, idxs in by_src.items():\n",
    "    r = random.Random(rng_state)\n",
    "    r.shuffle(idxs)\n",
    "    k = max(1, int(0.10 * len(idxs)))\n",
    "    hold_idx.update(idxs[:k])\n",
    "\n",
    "train_idx = [i for i in range(len(fp)) if i not in hold_idx]\n",
    "test_idx  = [i for i in range(len(fp)) if i in hold_idx]\n",
    "with open(os.path.join(OUTPUT_DIR, \"lda_split_FP_stratified.json\"), \"w\") as f:\n",
    "    json.dump({\"random_state\": rng_state, \"train_idx\": train_idx, \"test_idx\": test_idx}, f, indent=2)\n",
    "\n",
    "print(f\"ðŸ§ª Stratified split saved â€” Train: {len(train_idx)}  Test: {len(test_idx)}\")\n",
    "print(f\"âœ… Artifacts saved:\\n- Cleaned: {clean_path}\\n- Dictionary: {dict_path}\\n- Split: {os.path.join(OUTPUT_DIR, 'lda_split_FP_stratified.json')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LDA K sweep (FP, K = 1..35) â€” stratified split, CSV, and plots ===\n",
    "import os, math, json, random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# ---- Config ----\n",
    "INPUT_FILE      = os.path.join(\"..\", \"data\", \"final\", \"Filtered_Combined_FP_Cleaned.parquet\")\n",
    "OUT_DIR         = os.path.join(\"..\", \"data\", \"final\")\n",
    "DICT_PATH       = os.path.join(OUT_DIR, \"lda_dictionary_FP.dict\")  # FP dictionary\n",
    "K_GRID          = list(range(1, 36))  # 1..35 inclusive\n",
    "RANDOM_STATE    = 11\n",
    "PASSES, ITERS   = 5, 400              # can bump later when retraining best_k\n",
    "CHUNKSIZE       = 2000\n",
    "WORKERS         = os.cpu_count()\n",
    "\n",
    "RESULTS_CSV     = os.path.join(OUT_DIR, \"lda_k_selection_FP_metrics_1_35.csv\")\n",
    "SPLIT_JSON      = os.path.join(OUT_DIR, \"lda_split_FP_stratified.json\")\n",
    "PLOT_COMBINED   = os.path.join(OUT_DIR, \"lda_k_sweep_FP_1_35.png\")\n",
    "PLOT_COH_ONLY   = os.path.join(OUT_DIR, \"lda_k_sweep_FP_coherence_only.png\")\n",
    "PLOT_LP_ONLY    = os.path.join(OUT_DIR, \"lda_k_sweep_FP_logperp_only.png\")\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ---- Load ----\n",
    "print(\"ðŸ“‚ Loading data/dictionary...\")\n",
    "df = pd.read_parquet(INPUT_FILE)\n",
    "texts = df[\"tokens\"].tolist()\n",
    "sources = df[\"creator_or_game\"].tolist() if \"creator_or_game\" in df.columns else [\"ALL\"] * len(texts)\n",
    "\n",
    "dictionary = Dictionary.load(DICT_PATH)\n",
    "corpus = [dictionary.doc2bow(t) for t in texts]\n",
    "print(f\"âœ… docs={len(corpus)}  vocab={len(dictionary)}\")\n",
    "\n",
    "# ---- Stratified train/test split by source (90/10) ----\n",
    "if os.path.exists(SPLIT_JSON):\n",
    "    print(f\"ðŸ” Using existing split: {SPLIT_JSON}\")\n",
    "    with open(SPLIT_JSON, \"r\") as f:\n",
    "        split = json.load(f)\n",
    "    train_idx, test_idx = split[\"train_idx\"], split[\"test_idx\"]\n",
    "else:\n",
    "    print(\"ðŸ†• Creating stratified split (90/10 by source)...\")\n",
    "    rng = random.Random(RANDOM_STATE)\n",
    "    by_src = defaultdict(list)\n",
    "    for i, s in enumerate(sources):\n",
    "        by_src[s].append(i)\n",
    "\n",
    "    hold_idx = set()\n",
    "    for s, idxs in by_src.items():\n",
    "        rng.shuffle(idxs)\n",
    "        k = max(1, int(0.10 * len(idxs)))  # 10% per source\n",
    "        hold_idx.update(idxs[:k])\n",
    "\n",
    "    train_idx = [i for i in range(len(corpus)) if i not in hold_idx]\n",
    "    test_idx  = [i for i in range(len(corpus)) if i in hold_idx]\n",
    "\n",
    "    with open(SPLIT_JSON, \"w\") as f:\n",
    "        json.dump({\"random_state\": RANDOM_STATE, \"train_idx\": train_idx, \"test_idx\": test_idx}, f)\n",
    "\n",
    "train_corpus = [corpus[i] for i in train_idx]\n",
    "test_corpus  = [corpus[i] for i in test_idx]\n",
    "train_texts  = [texts[i] for i in train_idx]\n",
    "print(f\"ðŸ§ª Stratified split â€” Train: {len(train_corpus)}  Test: {len(test_corpus)}\")\n",
    "\n",
    "# ---- Train/eval helper ----\n",
    "def train_eval(k: int):\n",
    "    model = LdaMulticore(\n",
    "        corpus=train_corpus,\n",
    "        id2word=dictionary,\n",
    "        num_topics=k,\n",
    "        passes=PASSES,\n",
    "        iterations=ITERS,\n",
    "        random_state=RANDOM_STATE,\n",
    "        workers=WORKERS,\n",
    "        chunksize=CHUNKSIZE,\n",
    "        eval_every=None,\n",
    "        # Optional priors to try later:\n",
    "        # alpha='asymmetric', eta=None\n",
    "    )\n",
    "    # Coherence on TRAIN to avoid leakage\n",
    "    c_v = CoherenceModel(model=model, texts=train_texts, dictionary=dictionary, coherence=\"c_v\").get_coherence()\n",
    "    # Held-out log_perplexity: higher (less negative) is better\n",
    "    log_perp = model.log_perplexity(test_corpus)\n",
    "    return model, c_v, log_perp\n",
    "\n",
    "# ---- Sweep K ----\n",
    "rows = []\n",
    "best = {\"k\": None, \"c_v\": -math.inf, \"log_perplexity\": -math.inf, \"model\": None}\n",
    "\n",
    "for k in K_GRID:\n",
    "    print(f\"\\nâ³ Training LDA (k={k}) ...\")\n",
    "    model, c_v, log_perp = train_eval(k)\n",
    "    print(f\"ðŸ“ˆ k={k} | c_v={c_v:.4f} | log_perplexity={log_perp:.4f} (higher = better)\")\n",
    "    rows.append({\"k\": k, \"c_v\": c_v, \"log_perplexity\": log_perp})\n",
    "\n",
    "    # Best by highest c_v; tie-break by highest log_perplexity\n",
    "    if (c_v > best[\"c_v\"]) or (math.isclose(c_v, best[\"c_v\"], rel_tol=1e-6) and log_perp > best[\"log_perplexity\"]):\n",
    "        best.update({\"k\": k, \"c_v\": c_v, \"log_perplexity\": log_perp, \"model\": model})\n",
    "\n",
    "# ---- Save metrics table ----\n",
    "dfm = pd.DataFrame(rows).sort_values(\"k\")\n",
    "dfm.to_csv(RESULTS_CSV, index=False)\n",
    "print(f\"\\nðŸ“ Saved metrics -> {RESULTS_CSV}\")\n",
    "\n",
    "# ---- Save best model & topic terms ----\n",
    "best_k = best[\"k\"]\n",
    "best_model = best[\"model\"]\n",
    "best_path = os.path.join(OUT_DIR, f\"best_lda_model_FP_k{best_k}.model\")\n",
    "best_model.save(best_path)\n",
    "print(f\"ðŸ† Best K={best_k} | c_v={best['c_v']:.4f} | log_perplexity={best['log_perplexity']:.4f}\")\n",
    "print(f\"ðŸ’¾ Saved best model -> {best_path}\")\n",
    "\n",
    "def dump_topics(model, topn=20, path=None):\n",
    "    rows = []\n",
    "    for t in range(model.num_topics):\n",
    "        for rank, (w, p) in enumerate(model.show_topic(t, topn=topn), start=1):\n",
    "            rows.append({\"topic\": t, \"rank\": rank, \"word\": w, \"prob\": p})\n",
    "    dt = pd.DataFrame(rows)\n",
    "    if path: dt.to_csv(path, index=False)\n",
    "    return dt\n",
    "\n",
    "topics_csv = os.path.join(OUT_DIR, f\"best_topics_FP_k{best_k}.csv\")\n",
    "dump_topics(best_model, topn=20, path=topics_csv)\n",
    "print(f\"ðŸ—‚ï¸ Topic top-terms saved -> {topics_csv}\")\n",
    "\n",
    "# ---- Plots ----\n",
    "def plot_combined(df, best_k, out_path):\n",
    "    df = df.sort_values(\"k\")\n",
    "    fig, ax1 = plt.subplots(figsize=(9, 5))\n",
    "    ax1.plot(df[\"k\"], df[\"c_v\"], marker=\"o\", label=\"c_v\")\n",
    "    ax1.set_xlabel(\"K (number of topics)\")\n",
    "    ax1.set_ylabel(\"Coherence (c_v)\")\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(df[\"k\"], df[\"log_perplexity\"], marker=\"s\", linestyle=\"--\", label=\"log_perplexity\")\n",
    "    ax2.set_ylabel(\"log_perplexity (higher is better)\")\n",
    "\n",
    "    ax1.axvline(best_k, linestyle=\":\", linewidth=1.5)\n",
    "    ax1.set_title(f\"LDA K Sweep (K=1..35) â€” Best K={best_k}\")\n",
    "\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc=\"best\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"ðŸ–¼ï¸ Saved combined plot -> {out_path}\")\n",
    "\n",
    "def plot_single(x, y, ylabel, title, out_path, marker=\"o\"):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(x, y, marker=marker)\n",
    "    plt.xlabel(\"K (number of topics)\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"ðŸ–¼ï¸ Saved plot -> {out_path}\")\n",
    "\n",
    "# Combined (twin-axis)\n",
    "plot_combined(dfm, best_k, PLOT_COMBINED)\n",
    "\n",
    "# Separate per-metric plots\n",
    "plot_single(dfm[\"k\"], dfm[\"c_v\"], \"Coherence (c_v)\", \"LDA K Sweep â€” Coherence\", PLOT_COH_ONLY, marker=\"o\")\n",
    "plot_single(dfm[\"k\"], dfm[\"log_perplexity\"], \"log_perplexity (higher is better)\", \"LDA K Sweep â€” log_perplexity\", PLOT_LP_ONLY, marker=\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "OUT_DIR   = os.path.join(\"..\", \"data\", \"final\")\n",
    "INPUT     = os.path.join(OUT_DIR, \"Filtered_Combined_FP_Cleaned.parquet\")\n",
    "DICT_PATH = os.path.join(OUT_DIR, \"lda_dictionary_FP.dict\")\n",
    "\n",
    "BEST_K    = 3\n",
    "RND       = 11\n",
    "PASSES    = 20\n",
    "ITERS     = 1000\n",
    "CHUNKSIZE = 2000\n",
    "WORKERS   = os.cpu_count()\n",
    "\n",
    "print(\"ðŸ“‚ Loading full FP corpus/dictionary...\")\n",
    "df = pd.read_parquet(INPUT)\n",
    "texts = df[\"tokens\"].tolist()\n",
    "dictionary = Dictionary.load(DICT_PATH)\n",
    "corpus = [dictionary.doc2bow(t) for t in texts]\n",
    "print(f\"âœ… docs={len(corpus)}  vocab={len(dictionary)}\")\n",
    "\n",
    "print(f\"â³ Training FINAL FP model (K={BEST_K}) on ALL docs...\")\n",
    "final_model = LdaMulticore(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=BEST_K,\n",
    "    passes=PASSES,\n",
    "    iterations=ITERS,\n",
    "    random_state=RND,\n",
    "    workers=WORKERS,\n",
    "    chunksize=CHUNKSIZE,\n",
    "    eval_every=None,\n",
    "    alpha='asymmetric',\n",
    "    eta='auto'\n",
    ")\n",
    "\n",
    "# Save model\n",
    "final_model_path = os.path.join(OUT_DIR, f\"final_lda_FP_k{BEST_K}.model\")\n",
    "final_model.save(final_model_path)\n",
    "print(f\"ðŸ’¾ Saved final model -> {final_model_path}\")\n",
    "\n",
    "# Export top-10 terms per topic\n",
    "rows = []\n",
    "for t in range(BEST_K):\n",
    "    for rank, (w, p) in enumerate(final_model.show_topic(t, topn=10), start=1):\n",
    "        rows.append({\"topic\": t, \"rank\": rank, \"word\": w, \"prob\": p})\n",
    "topics_csv = os.path.join(OUT_DIR, f\"final_topics_FP_k{BEST_K}.csv\")\n",
    "pd.DataFrame(rows).to_csv(topics_csv, index=False)\n",
    "print(f\"ðŸ—‚ï¸ Topic words saved -> {topics_csv}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
