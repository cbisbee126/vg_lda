{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas nltk gensim pyarrow fastparquet matplotlib sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= POS-aware Cleaning Pipeline (All Games) =========\n",
    "import os, re, glob, json, pickle, random\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "# ----- Ensure NLTK resources -----\n",
    "for pkg in (\"stopwords\", \"punkt\", \"wordnet\"):\n",
    "    try:\n",
    "        nltk.data.find(f\"corpora/{pkg}\" if pkg != \"punkt\" else \"tokenizers/punkt\")\n",
    "    except LookupError:\n",
    "        nltk.download(pkg)\n",
    "\n",
    "# Tagger (try new name first, fallback to old)\n",
    "try:\n",
    "    nltk.data.find(\"taggers/averaged_perceptron_tagger_eng\")\n",
    "except LookupError:\n",
    "    try:\n",
    "        nltk.download(\"averaged_perceptron_tagger_eng\")\n",
    "    except:\n",
    "        nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "from nltk.corpus import stopwords, wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag_sents\n",
    "\n",
    "# ----- Config -----\n",
    "# Use relative paths so the code works for anyone from the repo root!\n",
    "INPUT_ROOTS = [\n",
    "    os.path.join(\"..\", \"data\", \"raw\")\n",
    "]\n",
    "OUT_DIR = os.path.join(\"..\", \"data\", \"final\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Flexible matching for *all* titles\n",
    "GLOB_PATTERNS = {\n",
    "    \"Legend of Zelda\":        \"Legend*Wild*Comments*Analysis.parquet\",\n",
    "    \"Fortnite_Ninja\":         \"Fortnite*Ninja*Comments*Analysis.parquet\",\n",
    "    \"Fortnite_SypherPK\":      \"Fortnite*Sypher*Comments*Analysis.parquet\",\n",
    "    \"Fortnite_NickEh30\":      \"Fortnite*Nick*Eh*30*Comments*Analysis.parquet\",\n",
    "    \"Apex Legends\":           \"Apex*Legends*Comments*Analysis.parquet\",\n",
    "    \"Baldur's Gate 3\":        \"Baldur*Gate*3*Comments*Analysis.parquet\",\n",
    "    \"Rocket League\":          \"Rocket*League*Comments*Analysis.parquet\",\n",
    "    \"Elden Ring\":             \"Elden*Ring*Comments*Analysis.parquet\",\n",
    "    \"Hollow Knight\":          \"Hollow*Knight*Comments*Analysis.parquet\",\n",
    "    \"Red Dead Redemption 2\":  \"Red*Dead*Redemption*2*Comments*Analysis.parquet\",\n",
    "    \"DOTA 2\":                 \"DOTA*2*Comments*Analysis.parquet\",\n",
    "    \"Valorant\":               \"Valorant*Comments*Analysis.parquet\",\n",
    "}\n",
    "\n",
    "# Phrase thresholds\n",
    "BIGRAM_MIN_COUNT = 5\n",
    "PHRASE_THRESHOLD = 8.0\n",
    "\n",
    "# Row min tokens\n",
    "MIN_TOKENS_ROW = 5\n",
    "\n",
    "# Dictionary pruning\n",
    "NO_BELOW = 5\n",
    "NO_ABOVE = 0.50\n",
    "KEEP_N   = 100_000\n",
    "\n",
    "# Keep or drop franchise tokens in the GLOBAL model\n",
    "KEEP_FRANCHISE_TOKENS = False  # set True to allow topics anchored on franchise names\n",
    "\n",
    "# ----- Stopwords -----\n",
    "NLTK_STOP = set(stopwords.words(\"english\"))\n",
    "CUSTOM_STOP = {\n",
    "    # general chat/meta\n",
    "    'video','game','online','youtube','series','pls','lol','omg','xd','people','thing',\n",
    "    'play','playing','make','time','love','look','want','think','watch','know','got','use','cant',\n",
    "    'going','never','ever','part','help','played','getting','doesnt','bad','pretty',\n",
    "    'show','fuck','shit','talk','went','comment','cool','amazing','seen','best','like','get','one',\n",
    "    'dont','would','first','really','see','also','way','guy','good','say','back','much','still','even',\n",
    "    'man','thats','need','bro','new','kid','every','always','could','said','please','youre','actually',\n",
    "    'didnt','feel','ive','dude','name','keep','gon','watching','everyone','hey','someone','made','come',\n",
    "    'great','give','well','fun','nice','let','right','day','friend','thought','work','mean','take','vid',\n",
    "    'lmao','lot','god','something','hope','put','cause','literally','since','next','hate','used','saying',\n",
    "    'funny','many','vids','tbh','wtf','ngl','hell','thank','thanks','maybe','already','oh','real','whole',\n",
    "    'two','old','hour','minute','top','last','final','big','small','long','short','fast','slow','soon','later',\n",
    "    'yeah','yall','wanna','wont','idk','guess','sometimes','isnt','easy','point','almost','behind','beginning',\n",
    "    'true','sure','place','reason','whats','talking','view','stream','watched','bruh','tho','thumbnail',\n",
    "    # platform/meta chatter\n",
    "    'sub','channel','content','clip','stream',\n",
    "    # creator/channel handles\n",
    "    'ninja','sypher','sypherpk','nick','nickeh','nickeh30','shroud','jonas','zylbrad','brad','arin','dan','delirious',\n",
    "    # ranked/MMR meta\n",
    "    'ranked','rank','season','matchmaking','mmr','elo',\n",
    "    # phrase residues\n",
    "    'can_t','so_much','feel_like','oh_yeah_oh_yeah','sea_of_thief_sea','of_thief',\n",
    "    # meme/noise\n",
    "    'wiggle_wiggle_wiggle_wiggle',\n",
    "    'episode','gonna','anyone','second','little','probably','without','everything',\n",
    "    'another','year','stuff','around','wish','life','stop','wait','tell','start',\n",
    "    'leave','hear','saw','call','change','remember','anyone','probably','maybe',\n",
    "    'anyway','already','yet','still','even','also','else','whole','point','true',\n",
    "    'real','finally','little','big','long','short','high','low','fast','slow',\n",
    "    # vague verbs\n",
    "    'try','find','get','got','make','take','put','use','using','see','look',\n",
    "    'watch','watching','know','think','say','said','want','need',\n",
    "}\n",
    "\n",
    "if not KEEP_FRANCHISE_TOKENS:\n",
    "    CUSTOM_STOP |= {\n",
    "        'fortnite','apex','valorant','rocket_league','dota','zelda','elden_ring','hollow_knight',\n",
    "        'red_dead_redemption','red_dead_redemption_2','baldur','baldur_gate','baldur_gate_3','rdr','rdr2'\n",
    "    }\n",
    "\n",
    "STOP_WORDS = NLTK_STOP.union(CUSTOM_STOP)\n",
    "\n",
    "# ----- Regex/helpers -----\n",
    "URL_RE   = re.compile(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\")\n",
    "HTML_RE  = re.compile(r\"<.*?>\")\n",
    "PUNC_RE  = re.compile(r\"[^\\w\\s]\")\n",
    "DIGIT_RE = re.compile(r\"\\d+\")\n",
    "WS_RE    = re.compile(r\"\\s+\")\n",
    "LEMM     = WordNetLemmatizer()\n",
    "\n",
    "# Optional: fix odd lemmas if you see them\n",
    "LEMMA_FIX = {\n",
    "    # 'bos':'boss',\n",
    "}\n",
    "\n",
    "# Drop junk phrase artifacts (e.g., broken contractions)\n",
    "BAD_PHRASE = re.compile(r'(^[a-z]_t$|^t_[a-z]$|^[a-z]_[a-z]$)')\n",
    "\n",
    "def _wn_pos(tag: str):\n",
    "    if not tag: return wn.NOUN\n",
    "    t = tag[0]\n",
    "    return wn.ADJ if t == 'J' else wn.VERB if t == 'V' else wn.NOUN if t == 'N' else wn.ADV if t == 'R' else wn.NOUN\n",
    "\n",
    "def normalize(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = URL_RE.sub(\" \", text)\n",
    "    text = HTML_RE.sub(\" \", text)\n",
    "    text = PUNC_RE.sub(\" \", text)\n",
    "    text = DIGIT_RE.sub(\" \", text)\n",
    "    text = WS_RE.sub(\" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def tokenize_simple(text: str):\n",
    "    return text.split()\n",
    "\n",
    "def pos_lemmatize(tokens):\n",
    "    if not tokens:\n",
    "        return []\n",
    "    tagged = list(pos_tag_sents([tokens]))[0]\n",
    "    return [LEMM.lemmatize(w, _wn_pos(tag)) for (w, tag) in tagged]\n",
    "\n",
    "def resolve_path(pattern, roots):\n",
    "    for root in roots:\n",
    "        matches = glob.glob(os.path.join(root, pattern))\n",
    "        if matches:\n",
    "            matches.sort(key=lambda p: os.path.getmtime(p), reverse=True)\n",
    "            return matches[0]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LDA K sweep (ALL GAMES, K = 1..35) â€” stratified split, CSV, and plots ===\n",
    "import os, math, json, random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# ---- Config ----\n",
    "# Use relative paths for portability!\n",
    "OUT_DIR         = os.path.join(\"..\", \"data\", \"final\")\n",
    "INPUT_FILE      = os.path.join(OUT_DIR, \"Filtered_Combined_AllGames_Cleaned.parquet\")\n",
    "DICT_PATH       = os.path.join(OUT_DIR, \"lda_dictionary_AllGames.dict\")\n",
    "\n",
    "K_GRID          = list(range(1, 36))  # 1..35 inclusive\n",
    "RANDOM_STATE    = 11\n",
    "PASSES, ITERS   = 5, 400\n",
    "CHUNKSIZE       = 2000\n",
    "WORKERS         = max(1, (os.cpu_count() or 1))\n",
    "\n",
    "RESULTS_CSV     = os.path.join(OUT_DIR, \"lda_k_selection_AllGames_metrics_1_35.csv\")\n",
    "SPLIT_JSON      = os.path.join(OUT_DIR, \"lda_split_AllGames_stratified.json\")\n",
    "PLOT_COMBINED   = os.path.join(OUT_DIR, \"lda_k_sweep_AllGames_1_35.png\")\n",
    "PLOT_COH_ONLY   = os.path.join(OUT_DIR, \"lda_k_sweep_AllGames_coherence_only.png\")\n",
    "PLOT_LP_ONLY    = os.path.join(OUT_DIR, \"lda_k_sweep_AllGames_logperp_only.png\")\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ---- Load ----\n",
    "print(\"ðŸ“‚ Loading data/dictionary...\")\n",
    "df = pd.read_parquet(INPUT_FILE)\n",
    "texts = df[\"tokens\"].tolist()\n",
    "games = df[\"game\"].tolist() if \"game\" in df.columns else [\"ALL\"] * len(texts)\n",
    "\n",
    "dictionary = Dictionary.load(DICT_PATH)\n",
    "corpus = [dictionary.doc2bow(t) for t in texts]\n",
    "print(f\"âœ… docs={len(corpus)}  vocab={len(dictionary)}\")\n",
    "\n",
    "# ---- Stratified train/test split by game (90/10) ----\n",
    "if os.path.exists(SPLIT_JSON):\n",
    "    print(f\"ðŸ” Using existing split: {SPLIT_JSON}\")\n",
    "    with open(SPLIT_JSON, \"r\") as f:\n",
    "        split = json.load(f)\n",
    "    train_idx, test_idx = split[\"train_idx\"], split[\"test_idx\"]\n",
    "else:\n",
    "    print(\"ðŸ†• Creating stratified split (90/10 by game)...\")\n",
    "    rng = random.Random(RANDOM_STATE)\n",
    "    by_game = defaultdict(list)\n",
    "    for i, g in enumerate(games):\n",
    "        by_game[g].append(i)\n",
    "\n",
    "    hold_idx = set()\n",
    "    for g, idxs in by_game.items():\n",
    "        rng.shuffle(idxs)\n",
    "        k = max(1, int(0.10 * len(idxs)))\n",
    "        hold_idx.update(idxs[:k])\n",
    "\n",
    "    train_idx = [i for i in range(len(corpus)) if i not in hold_idx]\n",
    "    test_idx  = [i for i in range(len(corpus)) if i in hold_idx]\n",
    "\n",
    "    with open(SPLIT_JSON, \"w\") as f:\n",
    "        json.dump({\"random_state\": RANDOM_STATE, \"train_idx\": train_idx, \"test_idx\": test_idx}, f)\n",
    "\n",
    "train_corpus = [corpus[i] for i in train_idx]\n",
    "test_corpus  = [corpus[i] for i in test_idx]\n",
    "train_texts  = [texts[i] for i in train_idx]\n",
    "print(f\"ðŸ§ª Stratified split â€” Train: {len(train_corpus)}  Test: {len(test_corpus)}\")\n",
    "\n",
    "# ---- Train/eval helper ----\n",
    "def train_eval(k: int):\n",
    "    model = LdaMulticore(\n",
    "        corpus=train_corpus,\n",
    "        id2word=dictionary,\n",
    "        num_topics=k,\n",
    "        passes=PASSES,\n",
    "        iterations=ITERS,\n",
    "        random_state=RANDOM_STATE,\n",
    "        workers=WORKERS,\n",
    "        chunksize=CHUNKSIZE,\n",
    "        eval_every=None,\n",
    "        alpha='asymmetric',\n",
    "        eta='auto',\n",
    "    )\n",
    "    # Coherence on TRAIN to avoid leakage\n",
    "    c_v = CoherenceModel(model=model, texts=train_texts, dictionary=dictionary, coherence=\"c_v\").get_coherence()\n",
    "    # Held-out log_perplexity on TEST: higher (less negative) is better\n",
    "    log_perp = model.log_perplexity(test_corpus)\n",
    "    return model, c_v, log_perp\n",
    "\n",
    "# ---- Sweep K ----\n",
    "rows = []\n",
    "best = {\"k\": None, \"c_v\": -math.inf, \"log_perplexity\": -math.inf, \"model\": None}\n",
    "\n",
    "for k in K_GRID:\n",
    "    print(f\"\\nâ³ Training LDA (k={k}) ...\")\n",
    "    model, c_v, log_perp = train_eval(k)\n",
    "    print(f\"ðŸ“ˆ k={k} | c_v={c_v:.4f} | log_perplexity={log_perp:.4f} (higher = better)\")\n",
    "    rows.append({\"k\": k, \"c_v\": c_v, \"log_perplexity\": log_perp})\n",
    "\n",
    "    # Best by highest c_v; tie-break by highest log_perplexity\n",
    "    if (c_v > best[\"c_v\"]) or (math.isclose(c_v, best[\"c_v\"], rel_tol=1e-6) and log_perp > best[\"log_perplexity\"]):\n",
    "        best.update({\"k\": k, \"c_v\": c_v, \"log_perplexity\": log_perp, \"model\": model})\n",
    "\n",
    "# ---- Save metrics table ----\n",
    "dfm = pd.DataFrame(rows).sort_values(\"k\")\n",
    "dfm.to_csv(RESULTS_CSV, index=False)\n",
    "print(f\"\\nðŸ“ Saved metrics -> {RESULTS_CSV}\")\n",
    "\n",
    "# ---- Save best model & topic terms ----\n",
    "best_k = best[\"k\"]\n",
    "best_model = best[\"model\"]\n",
    "best_path = os.path.join(OUT_DIR, f\"best_lda_model_AllGames_k{best_k}.model\")\n",
    "best_model.save(best_path)\n",
    "print(f\"ðŸ† Best K={best_k} | c_v={best['c_v']:.4f} | log_perplexity={best['log_perplexity']:.4f}\")\n",
    "print(f\"ðŸ’¾ Saved best model -> {best_path}\")\n",
    "\n",
    "def dump_topics(model, topn=20, path=None):\n",
    "    rows = []\n",
    "    for t in range(model.num_topics):\n",
    "        for rank, (w, p) in enumerate(model.show_topic(t, topn=topn), start=1):\n",
    "            rows.append({\"topic\": t, \"rank\": rank, \"word\": w, \"prob\": p})\n",
    "    dt = pd.DataFrame(rows)\n",
    "    if path: dt.to_csv(path, index=False)\n",
    "    return dt\n",
    "\n",
    "topics_csv = os.path.join(OUT_DIR, f\"best_topics_AllGames_k{best_k}.csv\")\n",
    "dump_topics(best_model, topn=20, path=topics_csv)\n",
    "print(f\"ðŸ—‚ï¸ Topic top-terms saved -> {topics_csv}\")\n",
    "\n",
    "# ---- Plots ----\n",
    "def plot_combined(df, best_k, out_path):\n",
    "    df = df.sort_values(\"k\")\n",
    "    fig, ax1 = plt.subplots(figsize=(9, 5))\n",
    "    ax1.plot(df[\"k\"], df[\"c_v\"], marker=\"o\", label=\"c_v\")\n",
    "    ax1.set_xlabel(\"K (number of topics)\")\n",
    "    ax1.set_ylabel(\"Coherence (c_v)\")\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(df[\"k\"], df[\"log_perplexity\"], marker=\"s\", linestyle=\"--\", label=\"log_perplexity\")\n",
    "    ax2.set_ylabel(\"log_perplexity (higher is better)\")\n",
    "\n",
    "    ax1.axvline(best_k, linestyle=\":\", linewidth=1.5)\n",
    "    ax1.set_title(f\"LDA K Sweep (All Games, K=1..35) â€” Best K={best_k}\")\n",
    "\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc=\"best\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"ðŸ–¼ï¸ Saved combined plot -> {out_path}\")\n",
    "\n",
    "def plot_single(x, y, ylabel, title, out_path, marker=\"o\"):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(x, y, marker=marker)\n",
    "    plt.xlabel(\"K (number of topics)\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"ðŸ–¼ï¸ Saved plot -> {out_path}\")\n",
    "\n",
    "# Combined (twin-axis)\n",
    "plot_combined(dfm, best_k, PLOT_COMBINED)\n",
    "\n",
    "# Separate per-metric plots\n",
    "plot_single(dfm[\"k\"], dfm[\"c_v\"], \"Coherence (c_v)\", \"LDA K Sweep â€” Coherence (All Games)\", PLOT_COH_ONLY, marker=\"o\")\n",
    "plot_single(dfm[\"k\"], dfm[\"log_perplexity\"], \"log_perplexity (higher is better)\", \"LDA K Sweep â€” log_perplexity (All Games)\", PLOT_LP_ONLY, marker=\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# ðŸ”¹ Step 1: Load Cleaned Tokenized Data\n",
    "lda_data_path = os.path.join(\"..\", \"data\", \"final\", \"Filtered_Combined_AllGames_Cleaned.parquet\")\n",
    "df = pd.read_parquet(lda_data_path)\n",
    "\n",
    "# ðŸ”¹ Step 2: Create Dictionary & Corpus\n",
    "dictionary = Dictionary(df[\"tokens\"])\n",
    "corpus = [dictionary.doc2bow(tokens) for tokens in df[\"tokens\"]]\n",
    "print(f\"âœ… Dictionary and corpus created with {len(dictionary)} unique tokens.\")\n",
    "\n",
    "# ðŸ”¹ Step 3: Define Function to Print Top Words for Each Topic\n",
    "def print_top_words(lda_model, topn=10):\n",
    "    k = lda_model.num_topics\n",
    "    print(f\"\\nðŸ”¹ Top {topn} Words per Topic (K = {k})\")\n",
    "    for topic_id in range(k):\n",
    "        words = [word for word, _ in lda_model.show_topic(topic_id, topn=topn)]\n",
    "        print(f\"Topic {topic_id + 1}: {', '.join(words)}\")\n",
    "\n",
    "# ðŸ”¹ Step 4: Run LDA for 4 Topics\n",
    "print(\"\\nðŸ”¹ Running LDA for 4 Topics...\")\n",
    "lda_model_4 = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=4,\n",
    "    iterations=5000,\n",
    "    random_state=42\n",
    ")\n",
    "print_top_words(lda_model_4, topn=10)\n",
    "\n",
    "# ðŸ”¹ Step 5: Run LDA for 5 Topics\n",
    "print(\"\\nðŸ”¹ Running LDA for 5 Topics...\")\n",
    "lda_model_5 = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=5,\n",
    "    iterations=5000,\n",
    "    random_state=42\n",
    ")\n",
    "print_top_words(lda_model_5, topn=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
