{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Colin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Fortnite_Ninja: 523501 rows â€” Fortnite_Ninja_Comments_Analysis.parquet\n",
      "âœ… Fortnite_SypherPK: 115364 rows â€” Fortnite_SypherPK_Comments_Analysis.parquet\n",
      "âœ… Fortnite_NickEh30: 180346 rows â€” Fortnite_NickEh30_Comments_Analysis.parquet\n",
      "âœ… Apex Legends: 486200 rows â€” Apex_Legends_Comments_Analysis.parquet\n",
      "âœ… Rocket League: 108567 rows â€” Rocket_League_Comments_Analysis.parquet\n",
      "âœ… DOTA 2: 10046 rows â€” DOTA_2_Comments_Analysis.parquet\n",
      "âœ… Valorant: 74291 rows â€” Valorant_Comments_Analysis.parquet\n",
      "ðŸ“Š Per-source counts: {'Fortnite_Ninja': 523501, 'Apex Legends': 486200, 'Fortnite_NickEh30': 180346, 'Fortnite_SypherPK': 115364, 'Rocket League': 108567, 'Valorant': 74291, 'DOTA 2': 10046}\n",
      "âœ… Removed 1243181 short comments (<5 tokens).\n",
      "ðŸ”¹ Top 50 tokens: [('kill', 18156), ('player', 17756), ('skin', 14340), ('win', 11616), ('try', 10698), ('start', 7851), ('diamond', 6657), ('plat', 6555), ('gold', 6540), ('gonna', 6476), ('die', 5833), ('hit', 5821), ('last', 5594), ('team', 5562), ('tell', 5529), ('call', 5248), ('max', 5189), ('miss', 4967), ('end', 4906), ('wait', 4719), ('buy', 4661), ('match', 4571), ('hard', 4549), ('anyone', 4449), ('find', 4416), ('lose', 4377), ('teammate', 4286), ('change', 3983), ('gun', 3968), ('epic', 3962), ('sea_of_thieves', 3936), ('silver', 3907), ('leave', 3865), ('stop', 3858), ('shot', 3846), ('hear', 3762), ('second', 3730), ('pro', 3705), ('year', 3684), ('wish', 3658), ('pathfinder', 3607), ('old', 3542), ('edit', 3507), ('maybe', 3503), ('saw', 3489), ('shoot', 3471), ('life', 3468), ('big', 3400), ('person', 3400), ('happy', 3387)]\n",
      "ðŸ’¾ Saved cleaned data -> C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\final\\Filtered_Combined_FP_Cleaned.parquet\n",
      "ðŸ“š Dictionary: 24088 tokens | Corpus docs: 255134\n",
      "ðŸ§ª Stratified split saved â€” Train: 229623  Test: 25511\n",
      "âœ… Artifacts saved:\n",
      "- Cleaned: C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\final\\Filtered_Combined_FP_Cleaned.parquet\n",
      "- Dictionary: C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\final\\lda_dictionary_FP.dict\n",
      "- Split: C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\final\\lda_split_FP_stratified.json\n"
     ]
    }
   ],
   "source": [
    "# ========= POS-aware Cleaning Pipeline (FP, adjusted) =========\n",
    "import os, re, glob, json, pickle, random\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "# ----- Ensure NLTK resources -----\n",
    "for pkg in (\"stopwords\", \"punkt\", \"wordnet\"):\n",
    "    try:\n",
    "        nltk.data.find(f\"corpora/{pkg}\" if pkg != \"punkt\" else \"tokenizers/punkt\")\n",
    "    except LookupError:\n",
    "        nltk.download(pkg)\n",
    "\n",
    "try:\n",
    "    nltk.data.find(\"taggers/averaged_perceptron_tagger_eng\")\n",
    "except LookupError:\n",
    "    try:\n",
    "        nltk.download(\"averaged_perceptron_tagger_eng\")\n",
    "    except:\n",
    "        nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "from nltk.corpus import stopwords, wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag_sents\n",
    "\n",
    "# ----- Config -----\n",
    "INPUT_ROOTS = [\n",
    "    r\"C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\raw\"\n",
    "]\n",
    "OUTPUT_DIR  = r\"C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\final\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "GLOB_PATTERNS = {\n",
    "    \"Fortnite_Ninja\":    \"Fortnite*Ninja*Comments*Analysis.parquet\",\n",
    "    \"Fortnite_SypherPK\": \"Fortnite*Sypher*Comments*Analysis.parquet\",\n",
    "    \"Fortnite_NickEh30\": \"Fortnite*Nick*Eh*30*Comments*Analysis.parquet\",\n",
    "    \"Apex Legends\":      \"Apex*Legends*Comments*Analysis.parquet\",\n",
    "    \"Rocket League\":     \"Rocket*League*Comments*Analysis.parquet\",\n",
    "    \"DOTA 2\":            \"DOTA*2*Comments*Analysis.parquet\",\n",
    "    \"Valorant\":          \"Valorant*Comments*Analysis.parquet\",\n",
    "}\n",
    "\n",
    "# Phrase thresholds (tightened for big corpus)\n",
    "BIGRAM_MIN_COUNT = 10\n",
    "PHRASE_THRESHOLD = 8.0\n",
    "\n",
    "# Row min tokens\n",
    "MIN_TOKENS_ROW   = 5\n",
    "\n",
    "# Dictionary pruning\n",
    "NO_BELOW = 5\n",
    "NO_ABOVE = 0.50\n",
    "KEEP_N   = 100_000\n",
    "\n",
    "# Keep or drop franchise tokens in the GLOBAL model\n",
    "KEEP_FRANCHISE_TOKENS = False  # set True if you want topics to explicitly include franchise names\n",
    "\n",
    "# ----- Stopwords -----\n",
    "NLTK_STOP = set(stopwords.words(\"english\"))\n",
    "CUSTOM_STOP = {\n",
    "    # general chat/meta\n",
    "    'video','game','online','youtube','series','pls','lol','omg','xd','people','thing',\n",
    "    'play','playing','make','time','love','look','want','think','watch','know','got','use','cant',\n",
    "    'going','never','ever','part','help','played','getting','doesnt','bad','pretty',\n",
    "    'show','fuck','shit','talk','went','comment','cool','amazing','seen','best','like','get','one',\n",
    "    'dont','would','first','really','see','also','way','guy','good','say','back','much','still','even',\n",
    "    'man','thats','need','bro','new','kid','every','always','could','said','please','youre','actually',\n",
    "    'didnt','feel','ive','dude','name','keep','gon','watching','everyone','hey','someone','made','come',\n",
    "    'great','give','well','fun','nice','let','right','day','friend','thought','work','mean','take','vid',\n",
    "    'lmao','lot','god','something','hope','put','cause','literally','since','next','hate','used','saying',\n",
    "    'funny','many','vids','tbh','wtf','ngl','hell',\n",
    "    # platform/meta chatter\n",
    "    'sub','channel',\n",
    "    # creator/channel handles\n",
    "    'ninja','sypher','sypherpk','nick','nickeh','nickeh30','shroud','jonas','zylbrad','brad',\n",
    "    # ranked/MMR/meta that often drowns semantics\n",
    "    'content','clip','stream','ranked','rank','season','matchmaking','mmr','elo',\n",
    "    # contractions / filler phrases surfaced by phrase model\n",
    "    'can_t','so_much',\n",
    "    # meme/noise seen in topics\n",
    "    'oh_yeah_oh_yeah','plz',\n",
    "    # low-signal color words often tied to skins\n",
    "    'red','blue',\n",
    "    # infamous meme token\n",
    "    'wiggle_wiggle_wiggle_wiggle',\n",
    "}\n",
    "if not KEEP_FRANCHISE_TOKENS:\n",
    "    CUSTOM_STOP |= {'fortnite','apex','valorant','rocket_league','dota'}\n",
    "\n",
    "STOP_WORDS = NLTK_STOP.union(CUSTOM_STOP)\n",
    "\n",
    "# ----- Regex/helpers -----\n",
    "URL_RE   = re.compile(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\")\n",
    "HTML_RE  = re.compile(r\"<.*?>\")\n",
    "PUNC_RE  = re.compile(r\"[^\\w\\s]\")\n",
    "DIGIT_RE = re.compile(r\"\\d+\")\n",
    "WS_RE    = re.compile(r\"\\s+\")\n",
    "LEMM     = WordNetLemmatizer()\n",
    "\n",
    "# Canonicalize known artifacts AFTER phrase expansion\n",
    "LEMMA_FIX = {\n",
    "    'of_thief': 'sea_of_thieves',\n",
    "    'sea_of_thief': 'sea_of_thieves',\n",
    "    'sea_of_thief_sea': 'sea_of_thieves',\n",
    "}\n",
    "\n",
    "# Remove two-letter underscore junk like \"t_s\", \"s_t\", etc.\n",
    "BAD_PHRASE = re.compile(r'^[a-z]_[a-z]$')\n",
    "\n",
    "def _wn_pos(tag: str):\n",
    "    if not tag: return wn.NOUN\n",
    "    t = tag[0]\n",
    "    return wn.ADJ if t == 'J' else wn.VERB if t == 'V' else wn.NOUN if t == 'N' else wn.ADV if t == 'R' else wn.NOUN\n",
    "\n",
    "def normalize(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = URL_RE.sub(\" \", text)\n",
    "    text = HTML_RE.sub(\" \", text)\n",
    "    text = PUNC_RE.sub(\" \", text)\n",
    "    text = DIGIT_RE.sub(\" \", text)\n",
    "    text = WS_RE.sub(\" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def tokenize_simple(text: str):\n",
    "    return text.split()\n",
    "\n",
    "def pos_lemmatize(tokens):\n",
    "    if not tokens:\n",
    "        return []\n",
    "    tagged = list(pos_tag_sents([tokens]))[0]\n",
    "    return [LEMM.lemmatize(w, _wn_pos(tag)) for (w, tag) in tagged]\n",
    "\n",
    "def resolve_path(pattern, roots):\n",
    "    for root in roots:\n",
    "        matches = glob.glob(os.path.join(root, pattern))\n",
    "        if matches:\n",
    "            matches.sort(key=lambda p: os.path.getmtime(p), reverse=True)\n",
    "            return matches[0]\n",
    "    return None\n",
    "\n",
    "# ---------- Load, normalize, POS-lemma ----------\n",
    "raw_dfs, missing = [], []\n",
    "for label, pat in GLOB_PATTERNS.items():\n",
    "    fpath = resolve_path(pat, INPUT_ROOTS)\n",
    "    if not fpath:\n",
    "        print(f\"âš ï¸ No match for {label} with pattern {pat} in {INPUT_ROOTS}\")\n",
    "        missing.append(label)\n",
    "        continue\n",
    "\n",
    "    df = pd.read_parquet(fpath)\n",
    "    if not {'author','text'}.issubset(df.columns):\n",
    "        print(f\"âš ï¸ Required columns missing in {os.path.basename(fpath)} â€” skipping.\")\n",
    "        continue\n",
    "\n",
    "    df = df.dropna(subset=['author','text']).copy()\n",
    "    df['__norm'] = df['text'].map(lambda t: normalize(t) if isinstance(t,str) else \"\")\n",
    "    df['__raw_tokens'] = df['__norm'].map(tokenize_simple)\n",
    "    df['raw_tokens'] = df['__raw_tokens'].map(pos_lemmatize)\n",
    "    df['creator_or_game'] = label\n",
    "\n",
    "    raw_dfs.append(df[['author','text','raw_tokens','creator_or_game']])\n",
    "    print(f\"âœ… {label}: {len(df)} rows â€” {os.path.basename(fpath)}\")\n",
    "\n",
    "if not raw_dfs:\n",
    "    raise SystemExit(\"No valid FP inputs loaded.\")\n",
    "\n",
    "fp = pd.concat(raw_dfs, ignore_index=True)\n",
    "print(\"ðŸ“Š Per-source counts:\", fp['creator_or_game'].value_counts().to_dict())\n",
    "\n",
    "# ---------- Train phrases on raw tokens (pre-stopwords) ----------\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "\n",
    "bigram  = Phrases(fp['raw_tokens'], min_count=BIGRAM_MIN_COUNT, threshold=PHRASE_THRESHOLD)\n",
    "trigram = Phrases(bigram[fp['raw_tokens']], threshold=PHRASE_THRESHOLD)\n",
    "bigram_phraser  = Phraser(bigram)\n",
    "trigram_phraser = Phraser(trigram)\n",
    "\n",
    "def apply_phrases_then_filter(toks):\n",
    "    phr = trigram_phraser[bigram_phraser[toks]]\n",
    "    # drop bad contraction-like artifacts\n",
    "    phr = [w for w in phr if not BAD_PHRASE.match(w)]\n",
    "    # lemma fix & stopword filter\n",
    "    phr = [LEMMA_FIX.get(w, w) for w in phr]\n",
    "    return [w for w in phr if w not in STOP_WORDS and len(w) > 2]\n",
    "\n",
    "fp['tokens'] = fp['raw_tokens'].apply(apply_phrases_then_filter)\n",
    "\n",
    "# ---------- Row-level min-length filter ----------\n",
    "initial = len(fp)\n",
    "fp = fp[fp['tokens'].str.len() >= MIN_TOKENS_ROW]\n",
    "print(f\"âœ… Removed {initial - len(fp)} short comments (<{MIN_TOKENS_ROW} tokens).\")\n",
    "\n",
    "# ---------- Peek tokens ----------\n",
    "all_tokens = [w for toks in fp['tokens'] for w in toks]\n",
    "print(\"ðŸ”¹ Top 50 tokens:\", Counter(all_tokens).most_common(50))\n",
    "\n",
    "# ---------- Save cleaned ----------\n",
    "clean_path = os.path.join(OUTPUT_DIR, \"Filtered_Combined_FP_Cleaned.parquet\")\n",
    "fp.to_parquet(clean_path, index=False)\n",
    "print(f\"ðŸ’¾ Saved cleaned data -> {clean_path}\")\n",
    "\n",
    "# ---------- Dictionary / Corpus (with pruning) ----------\n",
    "from gensim.corpora import Dictionary\n",
    "dictionary = Dictionary(fp['tokens'])\n",
    "dictionary.filter_extremes(no_below=NO_BELOW, no_above=NO_ABOVE, keep_n=KEEP_N)\n",
    "corpus = [dictionary.doc2bow(t) for t in fp['tokens']]\n",
    "print(f\"ðŸ“š Dictionary: {len(dictionary)} tokens | Corpus docs: {len(corpus)}\")\n",
    "\n",
    "dict_path = os.path.join(OUTPUT_DIR, \"lda_dictionary_FP.dict\")\n",
    "dictionary.save(dict_path)\n",
    "\n",
    "# Save phrasers & corpus for reuse\n",
    "bigram_phraser.save(os.path.join(OUTPUT_DIR, \"bigram_FP.pkl\"))\n",
    "trigram_phraser.save(os.path.join(OUTPUT_DIR, \"trigram_FP.pkl\"))\n",
    "with open(os.path.join(OUTPUT_DIR, \"lda_corpus_FP.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(corpus, f)\n",
    "\n",
    "# Save basic metadata\n",
    "with open(os.path.join(OUTPUT_DIR, \"cleaning_meta_FP.json\"), \"w\") as f:\n",
    "    json.dump({\n",
    "        \"no_below\": NO_BELOW,\n",
    "        \"no_above\": NO_ABOVE,\n",
    "        \"keep_n\": KEEP_N,\n",
    "        \"bigram_min_count\": BIGRAM_MIN_COUNT,\n",
    "        \"phrase_threshold\": PHRASE_THRESHOLD,\n",
    "        \"min_tokens_row\": MIN_TOKENS_ROW,\n",
    "        \"keep_franchise_tokens\": KEEP_FRANCHISE_TOKENS,\n",
    "        \"stopwords_sizes\": {\"nltk\": len(NLTK_STOP), \"custom\": len(CUSTOM_STOP)}\n",
    "    }, f, indent=2)\n",
    "\n",
    "# ---------- Stratified 90/10 split by source (creator/game) ----------\n",
    "rng_state = 11\n",
    "by_src = defaultdict(list)\n",
    "for i, src in enumerate(fp['creator_or_game']):\n",
    "    by_src[src].append(i)\n",
    "\n",
    "hold_idx = set()\n",
    "for src, idxs in by_src.items():\n",
    "    r = random.Random(rng_state)\n",
    "    r.shuffle(idxs)\n",
    "    k = max(1, int(0.10 * len(idxs)))\n",
    "    hold_idx.update(idxs[:k])\n",
    "\n",
    "train_idx = [i for i in range(len(fp)) if i not in hold_idx]\n",
    "test_idx  = [i for i in range(len(fp)) if i in hold_idx]\n",
    "with open(os.path.join(OUTPUT_DIR, \"lda_split_FP_stratified.json\"), \"w\") as f:\n",
    "    json.dump({\"random_state\": rng_state, \"train_idx\": train_idx, \"test_idx\": test_idx}, f, indent=2)\n",
    "\n",
    "print(f\"ðŸ§ª Stratified split saved â€” Train: {len(train_idx)}  Test: {len(test_idx)}\")\n",
    "print(f\"âœ… Artifacts saved:\\n- Cleaned: {clean_path}\\n- Dictionary: {dict_path}\\n- Split: {os.path.join(OUTPUT_DIR, 'lda_split_FP_stratified.json')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Loading data/dictionary...\n",
      "âœ… docs=255134  vocab=24088\n",
      "ðŸ” Using existing split: C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\final\\lda_split_FP_stratified.json\n",
      "ðŸ§ª Stratified split â€” Train: 229623  Test: 25511\n",
      "\n",
      "â³ Training LDA (k=1) ...\n",
      "ðŸ“ˆ k=1 | c_v=0.3936 | log_perplexity=-8.5964 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=2) ...\n",
      "ðŸ“ˆ k=2 | c_v=0.3918 | log_perplexity=-8.7823 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=3) ...\n",
      "ðŸ“ˆ k=3 | c_v=0.4269 | log_perplexity=-8.9442 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=4) ...\n",
      "ðŸ“ˆ k=4 | c_v=0.4259 | log_perplexity=-9.0595 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=5) ...\n",
      "ðŸ“ˆ k=5 | c_v=0.4066 | log_perplexity=-9.1571 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=6) ...\n",
      "ðŸ“ˆ k=6 | c_v=0.3859 | log_perplexity=-9.2452 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=7) ...\n",
      "ðŸ“ˆ k=7 | c_v=0.3665 | log_perplexity=-9.3323 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=8) ...\n",
      "ðŸ“ˆ k=8 | c_v=0.4072 | log_perplexity=-9.4475 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=9) ...\n",
      "ðŸ“ˆ k=9 | c_v=0.4065 | log_perplexity=-9.5951 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=10) ...\n",
      "ðŸ“ˆ k=10 | c_v=0.3896 | log_perplexity=-9.8074 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=11) ...\n",
      "ðŸ“ˆ k=11 | c_v=0.4030 | log_perplexity=-10.0120 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=12) ...\n",
      "ðŸ“ˆ k=12 | c_v=0.3979 | log_perplexity=-10.2233 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=13) ...\n",
      "ðŸ“ˆ k=13 | c_v=0.3879 | log_perplexity=-10.5403 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=14) ...\n",
      "ðŸ“ˆ k=14 | c_v=0.3988 | log_perplexity=-10.7932 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=15) ...\n",
      "ðŸ“ˆ k=15 | c_v=0.3884 | log_perplexity=-11.0886 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=16) ...\n",
      "ðŸ“ˆ k=16 | c_v=0.3838 | log_perplexity=-11.2472 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=17) ...\n",
      "ðŸ“ˆ k=17 | c_v=0.4062 | log_perplexity=-11.3593 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=18) ...\n",
      "ðŸ“ˆ k=18 | c_v=0.4015 | log_perplexity=-11.5038 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=19) ...\n",
      "ðŸ“ˆ k=19 | c_v=0.3667 | log_perplexity=-11.6277 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=20) ...\n",
      "ðŸ“ˆ k=20 | c_v=0.3841 | log_perplexity=-11.7408 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=21) ...\n",
      "ðŸ“ˆ k=21 | c_v=0.3891 | log_perplexity=-11.8688 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=22) ...\n",
      "ðŸ“ˆ k=22 | c_v=0.3687 | log_perplexity=-11.9925 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=23) ...\n",
      "ðŸ“ˆ k=23 | c_v=0.3773 | log_perplexity=-12.1133 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=24) ...\n",
      "ðŸ“ˆ k=24 | c_v=0.3778 | log_perplexity=-12.2275 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=25) ...\n",
      "ðŸ“ˆ k=25 | c_v=0.3778 | log_perplexity=-12.3510 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=26) ...\n",
      "ðŸ“ˆ k=26 | c_v=0.3886 | log_perplexity=-12.4548 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=27) ...\n",
      "ðŸ“ˆ k=27 | c_v=0.3795 | log_perplexity=-12.5737 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=28) ...\n",
      "ðŸ“ˆ k=28 | c_v=0.3740 | log_perplexity=-12.6857 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=29) ...\n",
      "ðŸ“ˆ k=29 | c_v=0.3796 | log_perplexity=-12.8032 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=30) ...\n",
      "ðŸ“ˆ k=30 | c_v=0.3754 | log_perplexity=-12.9219 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=31) ...\n",
      "ðŸ“ˆ k=31 | c_v=0.3625 | log_perplexity=-13.0466 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=32) ...\n",
      "ðŸ“ˆ k=32 | c_v=0.3752 | log_perplexity=-13.1509 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=33) ...\n",
      "ðŸ“ˆ k=33 | c_v=0.3662 | log_perplexity=-13.2754 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=34) ...\n",
      "ðŸ“ˆ k=34 | c_v=0.3650 | log_perplexity=-13.3810 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=35) ...\n",
      "ðŸ“ˆ k=35 | c_v=0.3560 | log_perplexity=-13.5052 (higher = better)\n",
      "\n",
      "ðŸ“ Saved metrics -> C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\final\\lda_k_selection_FP_metrics_1_35.csv\n",
      "ðŸ† Best K=3 | c_v=0.4269 | log_perplexity=-8.9442\n",
      "ðŸ’¾ Saved best model -> C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\final\\best_lda_model_FP_k3.model\n",
      "ðŸ—‚ï¸ Topic top-terms saved -> C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\final\\best_topics_FP_k3.csv\n",
      "ðŸ–¼ï¸ Saved combined plot -> C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\final\\lda_k_sweep_FP_1_35.png\n",
      "ðŸ–¼ï¸ Saved plot -> C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\final\\lda_k_sweep_FP_coherence_only.png\n",
      "ðŸ–¼ï¸ Saved plot -> C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\final\\lda_k_sweep_FP_logperp_only.png\n"
     ]
    }
   ],
   "source": [
    "# === LDA K sweep (FP, K = 1..35) â€” stratified split, CSV, and plots ===\n",
    "import os, math, json, random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# ---- Config ----\n",
    "INPUT_FILE      = r\"C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\final\\Filtered_Combined_FP_Cleaned.parquet\"\n",
    "OUT_DIR         = r\"C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\final\"\n",
    "DICT_PATH       = os.path.join(OUT_DIR, \"lda_dictionary_FP.dict\")  # FP dictionary\n",
    "K_GRID          = list(range(1, 36))  # 1..35 inclusive\n",
    "RANDOM_STATE    = 11\n",
    "PASSES, ITERS   = 5, 400              # can bump later when retraining best_k\n",
    "CHUNKSIZE       = 2000\n",
    "WORKERS         = os.cpu_count()\n",
    "\n",
    "RESULTS_CSV     = os.path.join(OUT_DIR, \"lda_k_selection_FP_metrics_1_35.csv\")\n",
    "SPLIT_JSON      = os.path.join(OUT_DIR, \"lda_split_FP_stratified.json\")\n",
    "PLOT_COMBINED   = os.path.join(OUT_DIR, \"lda_k_sweep_FP_1_35.png\")\n",
    "PLOT_COH_ONLY   = os.path.join(OUT_DIR, \"lda_k_sweep_FP_coherence_only.png\")\n",
    "PLOT_LP_ONLY    = os.path.join(OUT_DIR, \"lda_k_sweep_FP_logperp_only.png\")\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ---- Load ----\n",
    "print(\"ðŸ“‚ Loading data/dictionary...\")\n",
    "df = pd.read_parquet(INPUT_FILE)\n",
    "texts = df[\"tokens\"].tolist()\n",
    "sources = df[\"creator_or_game\"].tolist() if \"creator_or_game\" in df.columns else [\"ALL\"] * len(texts)\n",
    "\n",
    "dictionary = Dictionary.load(DICT_PATH)\n",
    "corpus = [dictionary.doc2bow(t) for t in texts]\n",
    "print(f\"âœ… docs={len(corpus)}  vocab={len(dictionary)}\")\n",
    "\n",
    "# ---- Stratified train/test split by source (90/10) ----\n",
    "if os.path.exists(SPLIT_JSON):\n",
    "    print(f\"ðŸ” Using existing split: {SPLIT_JSON}\")\n",
    "    with open(SPLIT_JSON, \"r\") as f:\n",
    "        split = json.load(f)\n",
    "    train_idx, test_idx = split[\"train_idx\"], split[\"test_idx\"]\n",
    "else:\n",
    "    print(\"ðŸ†• Creating stratified split (90/10 by source)...\")\n",
    "    rng = random.Random(RANDOM_STATE)\n",
    "    by_src = defaultdict(list)\n",
    "    for i, s in enumerate(sources):\n",
    "        by_src[s].append(i)\n",
    "\n",
    "    hold_idx = set()\n",
    "    for s, idxs in by_src.items():\n",
    "        rng.shuffle(idxs)\n",
    "        k = max(1, int(0.10 * len(idxs)))  # 10% per source\n",
    "        hold_idx.update(idxs[:k])\n",
    "\n",
    "    train_idx = [i for i in range(len(corpus)) if i not in hold_idx]\n",
    "    test_idx  = [i for i in range(len(corpus)) if i in hold_idx]\n",
    "\n",
    "    with open(SPLIT_JSON, \"w\") as f:\n",
    "        json.dump({\"random_state\": RANDOM_STATE, \"train_idx\": train_idx, \"test_idx\": test_idx}, f)\n",
    "\n",
    "train_corpus = [corpus[i] for i in train_idx]\n",
    "test_corpus  = [corpus[i] for i in test_idx]\n",
    "train_texts  = [texts[i] for i in train_idx]\n",
    "print(f\"ðŸ§ª Stratified split â€” Train: {len(train_corpus)}  Test: {len(test_corpus)}\")\n",
    "\n",
    "# ---- Train/eval helper ----\n",
    "def train_eval(k: int):\n",
    "    model = LdaMulticore(\n",
    "        corpus=train_corpus,\n",
    "        id2word=dictionary,\n",
    "        num_topics=k,\n",
    "        passes=PASSES,\n",
    "        iterations=ITERS,\n",
    "        random_state=RANDOM_STATE,\n",
    "        workers=WORKERS,\n",
    "        chunksize=CHUNKSIZE,\n",
    "        eval_every=None,\n",
    "        # Optional priors to try later:\n",
    "        # alpha='asymmetric', eta=None\n",
    "    )\n",
    "    # Coherence on TRAIN to avoid leakage\n",
    "    c_v = CoherenceModel(model=model, texts=train_texts, dictionary=dictionary, coherence=\"c_v\").get_coherence()\n",
    "    # Held-out log_perplexity: higher (less negative) is better\n",
    "    log_perp = model.log_perplexity(test_corpus)\n",
    "    return model, c_v, log_perp\n",
    "\n",
    "# ---- Sweep K ----\n",
    "rows = []\n",
    "best = {\"k\": None, \"c_v\": -math.inf, \"log_perplexity\": -math.inf, \"model\": None}\n",
    "\n",
    "for k in K_GRID:\n",
    "    print(f\"\\nâ³ Training LDA (k={k}) ...\")\n",
    "    model, c_v, log_perp = train_eval(k)\n",
    "    print(f\"ðŸ“ˆ k={k} | c_v={c_v:.4f} | log_perplexity={log_perp:.4f} (higher = better)\")\n",
    "    rows.append({\"k\": k, \"c_v\": c_v, \"log_perplexity\": log_perp})\n",
    "\n",
    "    # Best by highest c_v; tie-break by highest log_perplexity\n",
    "    if (c_v > best[\"c_v\"]) or (math.isclose(c_v, best[\"c_v\"], rel_tol=1e-6) and log_perp > best[\"log_perplexity\"]):\n",
    "        best.update({\"k\": k, \"c_v\": c_v, \"log_perplexity\": log_perp, \"model\": model})\n",
    "\n",
    "# ---- Save metrics table ----\n",
    "dfm = pd.DataFrame(rows).sort_values(\"k\")\n",
    "dfm.to_csv(RESULTS_CSV, index=False)\n",
    "print(f\"\\nðŸ“ Saved metrics -> {RESULTS_CSV}\")\n",
    "\n",
    "# ---- Save best model & topic terms ----\n",
    "best_k = best[\"k\"]\n",
    "best_model = best[\"model\"]\n",
    "best_path = os.path.join(OUT_DIR, f\"best_lda_model_FP_k{best_k}.model\")\n",
    "best_model.save(best_path)\n",
    "print(f\"ðŸ† Best K={best_k} | c_v={best['c_v']:.4f} | log_perplexity={best['log_perplexity']:.4f}\")\n",
    "print(f\"ðŸ’¾ Saved best model -> {best_path}\")\n",
    "\n",
    "def dump_topics(model, topn=20, path=None):\n",
    "    rows = []\n",
    "    for t in range(model.num_topics):\n",
    "        for rank, (w, p) in enumerate(model.show_topic(t, topn=topn), start=1):\n",
    "            rows.append({\"topic\": t, \"rank\": rank, \"word\": w, \"prob\": p})\n",
    "    dt = pd.DataFrame(rows)\n",
    "    if path: dt.to_csv(path, index=False)\n",
    "    return dt\n",
    "\n",
    "topics_csv = os.path.join(OUT_DIR, f\"best_topics_FP_k{best_k}.csv\")\n",
    "dump_topics(best_model, topn=20, path=topics_csv)\n",
    "print(f\"ðŸ—‚ï¸ Topic top-terms saved -> {topics_csv}\")\n",
    "\n",
    "# ---- Plots ----\n",
    "def plot_combined(df, best_k, out_path):\n",
    "    df = df.sort_values(\"k\")\n",
    "    fig, ax1 = plt.subplots(figsize=(9, 5))\n",
    "    ax1.plot(df[\"k\"], df[\"c_v\"], marker=\"o\", label=\"c_v\")\n",
    "    ax1.set_xlabel(\"K (number of topics)\")\n",
    "    ax1.set_ylabel(\"Coherence (c_v)\")\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(df[\"k\"], df[\"log_perplexity\"], marker=\"s\", linestyle=\"--\", label=\"log_perplexity\")\n",
    "    ax2.set_ylabel(\"log_perplexity (higher is better)\")\n",
    "\n",
    "    ax1.axvline(best_k, linestyle=\":\", linewidth=1.5)\n",
    "    ax1.set_title(f\"LDA K Sweep (K=1..35) â€” Best K={best_k}\")\n",
    "\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc=\"best\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"ðŸ–¼ï¸ Saved combined plot -> {out_path}\")\n",
    "\n",
    "def plot_single(x, y, ylabel, title, out_path, marker=\"o\"):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(x, y, marker=marker)\n",
    "    plt.xlabel(\"K (number of topics)\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"ðŸ–¼ï¸ Saved plot -> {out_path}\")\n",
    "\n",
    "# Combined (twin-axis)\n",
    "plot_combined(dfm, best_k, PLOT_COMBINED)\n",
    "\n",
    "# Separate per-metric plots\n",
    "plot_single(dfm[\"k\"], dfm[\"c_v\"], \"Coherence (c_v)\", \"LDA K Sweep â€” Coherence\", PLOT_COH_ONLY, marker=\"o\")\n",
    "plot_single(dfm[\"k\"], dfm[\"log_perplexity\"], \"log_perplexity (higher is better)\", \"LDA K Sweep â€” log_perplexity\", PLOT_LP_ONLY, marker=\"s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Loading full FP corpus/dictionary...\n",
      "âœ… docs=255134  vocab=24088\n",
      "â³ Training FINAL FP model (K=3) on ALL docs...\n",
      "ðŸ’¾ Saved final model -> C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\final\\final_lda_FP_k3.model\n",
      "ðŸ—‚ï¸ Topic words saved -> C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\final\\final_topics_FP_k3.csv\n"
     ]
    }
   ],
   "source": [
    "import os, pandas as pd\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "OUT_DIR   = r\"C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\final\"\n",
    "INPUT     = os.path.join(OUT_DIR, \"Filtered_Combined_FP_Cleaned.parquet\")\n",
    "DICT_PATH = os.path.join(OUT_DIR, \"lda_dictionary_FP.dict\")\n",
    "\n",
    "BEST_K    = 3\n",
    "RND       = 11\n",
    "PASSES    = 20\n",
    "ITERS     = 1000\n",
    "CHUNKSIZE = 2000\n",
    "WORKERS   = os.cpu_count()\n",
    "\n",
    "print(\"ðŸ“‚ Loading full FP corpus/dictionary...\")\n",
    "df = pd.read_parquet(INPUT)\n",
    "texts = df[\"tokens\"].tolist()\n",
    "dictionary = Dictionary.load(DICT_PATH)\n",
    "corpus = [dictionary.doc2bow(t) for t in texts]\n",
    "print(f\"âœ… docs={len(corpus)}  vocab={len(dictionary)}\")\n",
    "\n",
    "print(f\"â³ Training FINAL FP model (K={BEST_K}) on ALL docs...\")\n",
    "final_model = LdaMulticore(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=BEST_K,\n",
    "    passes=PASSES,\n",
    "    iterations=ITERS,\n",
    "    random_state=RND,\n",
    "    workers=WORKERS,\n",
    "    chunksize=CHUNKSIZE,\n",
    "    eval_every=None,\n",
    "    alpha='asymmetric',\n",
    "    eta='auto'\n",
    ")\n",
    "\n",
    "# Save model\n",
    "final_model_path = os.path.join(OUT_DIR, f\"final_lda_FP_k{BEST_K}.model\")\n",
    "final_model.save(final_model_path)\n",
    "print(f\"ðŸ’¾ Saved final model -> {final_model_path}\")\n",
    "\n",
    "# Export top-10 terms per topic\n",
    "rows = []\n",
    "for t in range(BEST_K):\n",
    "    for rank, (w, p) in enumerate(final_model.show_topic(t, topn=10), start=1):\n",
    "        rows.append({\"topic\": t, \"rank\": rank, \"word\": w, \"prob\": p})\n",
    "topics_csv = os.path.join(OUT_DIR, f\"final_topics_FP_k{BEST_K}.csv\")\n",
    "pd.DataFrame(rows).to_csv(topics_csv, index=False)\n",
    "print(f\"ðŸ—‚ï¸ Topic words saved -> {topics_csv}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
