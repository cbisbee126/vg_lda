{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\colin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: nltk in c:\\users\\colin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: gensim in c:\\users\\colin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\colin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (21.0.0)\n",
      "Requirement already satisfied: fastparquet in c:\\users\\colin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2024.11.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\colin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.10.7)\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Ã— python setup.py egg_info did not run successfully.\n",
      "  â”‚ exit code: 1\n",
      "  â•°â”€> [15 lines of output]\n",
      "      The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "      rather than 'sklearn' for pip commands.\n",
      "      \n",
      "      Here is how to fix this error in the main use cases:\n",
      "      - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "      - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "        (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "      - if the 'sklearn' package is used by one of your dependencies,\n",
      "        it would be great if you take some time to track which package uses\n",
      "        'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "      - as a last resort, set the environment variable\n",
      "        SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "      \n",
      "      More information is available at\n",
      "      https://github.com/scikit-learn/sklearn-pypi-package\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Ã— Encountered error while generating package metadata.\n",
      "â•°â”€> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n",
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pandas nltk gensim pyarrow fastparquet matplotlib sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Colin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Legend of Zelda: 201465 rows â€” Legend_of_Zelda_Breath_of_the_Wild_Comments_Analysis.parquet\n",
      "âœ… Fortnite_Ninja: 523501 rows â€” Fortnite_Ninja_Comments_Analysis.parquet\n",
      "âœ… Fortnite_SypherPK: 115364 rows â€” Fortnite_SypherPK_Comments_Analysis.parquet\n",
      "âœ… Fortnite_NickEh30: 180346 rows â€” Fortnite_NickEh30_Comments_Analysis.parquet\n",
      "âœ… Apex Legends: 486200 rows â€” Apex_Legends_Comments_Analysis.parquet\n",
      "âœ… Baldur's Gate 3: 11097 rows â€” Baldur's_Gate_3_Comments_Analysis.parquet\n",
      "âœ… Rocket League: 108567 rows â€” Rocket_League_Comments_Analysis.parquet\n",
      "âœ… Elden Ring: 129966 rows â€” Elden_Ring_Comments_Analysis.parquet\n",
      "âœ… Hollow Knight: 55429 rows â€” Hollow_Knight_Comments_Analysis.parquet\n",
      "âœ… Red Dead Redemption 2: 183118 rows â€” Red_Dead_Redemption_2_Comments_Analysis.parquet\n",
      "âœ… DOTA 2: 10046 rows â€” DOTA_2_Comments_Analysis.parquet\n",
      "âœ… Valorant: 74291 rows â€” Valorant_Comments_Analysis.parquet\n",
      "ðŸ“Š Per-game counts: {'Fortnite_Ninja': 523501, 'Apex Legends': 486200, 'Legend of Zelda': 201465, 'Red Dead Redemption 2': 183118, 'Fortnite_NickEh30': 180346, 'Elden Ring': 129966, 'Fortnite_SypherPK': 115364, 'Rocket League': 108567, 'Valorant': 74291, 'Hollow Knight': 55429, \"Baldur's Gate 3\": 11097, 'DOTA 2': 10046}\n",
      "âœ… Removed 1656358 short comments (<5 tokens).\n",
      "ðŸ”¹ Top 50 tokens: [('kill', 31211), ('player', 17234), ('end', 15421), ('fight', 14962), ('die', 14184), ('skin', 13490), ('hit', 13105), ('win', 12021), ('miss', 11253), ('hard', 10160), ('beat', 9640), ('arthur', 8533), ('buy', 8055), ('horse', 7707), ('weapon', 7521), ('lose', 7388), ('gold', 7273), ('enemy', 7004), ('shoot', 6755), ('bos', 6745), ('run', 6295), ('character', 6159), ('damage', 6156), ('map', 6103), ('plat', 6068), ('diamond', 6052), ('team', 5902), ('mission', 5900), ('level', 5837), ('seem', 5773), ('jack', 5734), ('world', 5728), ('follow', 5715), ('might', 5693), ('break', 5689), ('build', 5669), ('anything', 5615), ('person', 5580), ('merg', 5572), ('attack', 5564), ('add', 5538), ('though', 5446), ('edit', 5444), ('awesome', 5430), ('john', 5335), ('save', 5302), ('happen', 5298), ('damn', 5238), ('shot', 5229), ('drop', 5220)]\n",
      "ðŸ’¾ Saved cleaned data -> C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\final\\Filtered_Combined_AllGames_Cleaned.parquet\n",
      "ðŸ“š Dictionary: 40194 tokens | Corpus docs: 423032\n",
      "ðŸ§ª Stratified split saved â€” Train: 380734  Test: 42298\n",
      "âœ… Artifacts saved:\n",
      "- Cleaned: C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\final\\Filtered_Combined_AllGames_Cleaned.parquet\n",
      "- Dictionary: C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\final\\lda_dictionary_AllGames.dict\n",
      "- Split: C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\final\\lda_split_AllGames_stratified.json\n"
     ]
    }
   ],
   "source": [
    "# ========= POS-aware Cleaning Pipeline (All Games) =========\n",
    "import os, re, glob, json, pickle, random\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "# ----- Ensure NLTK resources -----\n",
    "for pkg in (\"stopwords\", \"punkt\", \"wordnet\"):\n",
    "    try:\n",
    "        nltk.data.find(f\"corpora/{pkg}\" if pkg != \"punkt\" else \"tokenizers/punkt\")\n",
    "    except LookupError:\n",
    "        nltk.download(pkg)\n",
    "\n",
    "# Tagger (try new name first, fallback to old)\n",
    "try:\n",
    "    nltk.data.find(\"taggers/averaged_perceptron_tagger_eng\")\n",
    "except LookupError:\n",
    "    try:\n",
    "        nltk.download(\"averaged_perceptron_tagger_eng\")\n",
    "    except:\n",
    "        nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "from nltk.corpus import stopwords, wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag_sents\n",
    "\n",
    "# ----- Config -----\n",
    "INPUT_ROOTS = [\n",
    "    r\"C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\raw\"\n",
    "]\n",
    "OUT_DIR = r\"C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\final\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Flexible matching for *all* titles\n",
    "GLOB_PATTERNS = {\n",
    "    \"Legend of Zelda\":        \"Legend*Wild*Comments*Analysis.parquet\",\n",
    "    \"Fortnite_Ninja\":         \"Fortnite*Ninja*Comments*Analysis.parquet\",\n",
    "    \"Fortnite_SypherPK\":      \"Fortnite*Sypher*Comments*Analysis.parquet\",\n",
    "    \"Fortnite_NickEh30\":      \"Fortnite*Nick*Eh*30*Comments*Analysis.parquet\",\n",
    "    \"Apex Legends\":           \"Apex*Legends*Comments*Analysis.parquet\",\n",
    "    \"Baldur's Gate 3\":        \"Baldur*Gate*3*Comments*Analysis.parquet\",\n",
    "    \"Rocket League\":          \"Rocket*League*Comments*Analysis.parquet\",\n",
    "    \"Elden Ring\":             \"Elden*Ring*Comments*Analysis.parquet\",\n",
    "    \"Hollow Knight\":          \"Hollow*Knight*Comments*Analysis.parquet\",\n",
    "    \"Red Dead Redemption 2\":  \"Red*Dead*Redemption*2*Comments*Analysis.parquet\",\n",
    "    \"DOTA 2\":                 \"DOTA*2*Comments*Analysis.parquet\",\n",
    "    \"Valorant\":               \"Valorant*Comments*Analysis.parquet\",\n",
    "}\n",
    "\n",
    "# Phrase thresholds\n",
    "BIGRAM_MIN_COUNT = 5\n",
    "PHRASE_THRESHOLD = 8.0\n",
    "\n",
    "# Row min tokens\n",
    "MIN_TOKENS_ROW = 5\n",
    "\n",
    "# Dictionary pruning\n",
    "NO_BELOW = 5\n",
    "NO_ABOVE = 0.50\n",
    "KEEP_N   = 100_000\n",
    "\n",
    "# Keep or drop franchise tokens in the GLOBAL model\n",
    "KEEP_FRANCHISE_TOKENS = False  # set True to allow topics anchored on franchise names\n",
    "\n",
    "# ----- Stopwords -----\n",
    "NLTK_STOP = set(stopwords.words(\"english\"))\n",
    "CUSTOM_STOP = {\n",
    "    # general chat/meta\n",
    "    'video','game','online','youtube','series','pls','lol','omg','xd','people','thing',\n",
    "    'play','playing','make','time','love','look','want','think','watch','know','got','use','cant',\n",
    "    'going','never','ever','part','help','played','getting','doesnt','bad','pretty',\n",
    "    'show','fuck','shit','talk','went','comment','cool','amazing','seen','best','like','get','one',\n",
    "    'dont','would','first','really','see','also','way','guy','good','say','back','much','still','even',\n",
    "    'man','thats','need','bro','new','kid','every','always','could','said','please','youre','actually',\n",
    "    'didnt','feel','ive','dude','name','keep','gon','watching','everyone','hey','someone','made','come',\n",
    "    'great','give','well','fun','nice','let','right','day','friend','thought','work','mean','take','vid',\n",
    "    'lmao','lot','god','something','hope','put','cause','literally','since','next','hate','used','saying',\n",
    "    'funny','many','vids','tbh','wtf','ngl','hell','thank','thanks','maybe','already','oh','real','whole',\n",
    "    'two','old','hour','minute','top','last','final','big','small','long','short','fast','slow','soon','later',\n",
    "    'yeah','yall','wanna','wont','idk','guess','sometimes','isnt','easy','point','almost','behind','beginning',\n",
    "    'true','sure','place','reason','whats','talking','view','stream','watched','bruh','tho','thumbnail',\n",
    "    # platform/meta chatter\n",
    "    'sub','channel','content','clip','stream',\n",
    "    # creator/channel handles\n",
    "    'ninja','sypher','sypherpk','nick','nickeh','nickeh30','shroud','jonas','zylbrad','brad','arin','dan','delirious',\n",
    "    # ranked/MMR meta\n",
    "    'ranked','rank','season','matchmaking','mmr','elo',\n",
    "    # phrase residues\n",
    "    'can_t','so_much','feel_like','oh_yeah_oh_yeah','sea_of_thief_sea','of_thief',\n",
    "    # meme/noise\n",
    "    'wiggle_wiggle_wiggle_wiggle',\n",
    "    'episode','gonna','anyone','second','little','probably','without','everything',\n",
    "    'another','year','stuff','around','wish','life','stop','wait','tell','start',\n",
    "    'leave','hear','saw','call','change','remember','anyone','probably','maybe',\n",
    "    'anyway','already','yet','still','even','also','else','whole','point','true',\n",
    "    'real','finally','little','big','long','short','high','low','fast','slow',\n",
    "    # vague verbs\n",
    "    'try','find','get','got','make','take','put','use','using','see','look',\n",
    "    'watch','watching','know','think','say','said','want','need',\n",
    "}\n",
    "\n",
    "if not KEEP_FRANCHISE_TOKENS:\n",
    "    CUSTOM_STOP |= {\n",
    "        'fortnite','apex','valorant','rocket_league','dota','zelda','elden_ring','hollow_knight',\n",
    "        'red_dead_redemption','red_dead_redemption_2','baldur','baldur_gate','baldur_gate_3','rdr','rdr2'\n",
    "    }\n",
    "\n",
    "STOP_WORDS = NLTK_STOP.union(CUSTOM_STOP)\n",
    "\n",
    "# ----- Regex/helpers -----\n",
    "URL_RE   = re.compile(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\")\n",
    "HTML_RE  = re.compile(r\"<.*?>\")\n",
    "PUNC_RE  = re.compile(r\"[^\\w\\s]\")\n",
    "DIGIT_RE = re.compile(r\"\\d+\")\n",
    "WS_RE    = re.compile(r\"\\s+\")\n",
    "LEMM     = WordNetLemmatizer()\n",
    "\n",
    "# Optional: fix odd lemmas if you see them\n",
    "LEMMA_FIX = {\n",
    "    # 'bos':'boss',\n",
    "}\n",
    "\n",
    "# Drop junk phrase artifacts (e.g., broken contractions)\n",
    "BAD_PHRASE = re.compile(r'(^[a-z]_t$|^t_[a-z]$|^[a-z]_[a-z]$)')\n",
    "\n",
    "def _wn_pos(tag: str):\n",
    "    if not tag: return wn.NOUN\n",
    "    t = tag[0]\n",
    "    return wn.ADJ if t == 'J' else wn.VERB if t == 'V' else wn.NOUN if t == 'N' else wn.ADV if t == 'R' else wn.NOUN\n",
    "\n",
    "def normalize(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = URL_RE.sub(\" \", text)\n",
    "    text = HTML_RE.sub(\" \", text)\n",
    "    text = PUNC_RE.sub(\" \", text)\n",
    "    text = DIGIT_RE.sub(\" \", text)\n",
    "    text = WS_RE.sub(\" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def tokenize_simple(text: str):\n",
    "    return text.split()\n",
    "\n",
    "def pos_lemmatize(tokens):\n",
    "    if not tokens:\n",
    "        return []\n",
    "    tagged = list(pos_tag_sents([tokens]))[0]\n",
    "    return [LEMM.lemmatize(w, _wn_pos(tag)) for (w, tag) in tagged]\n",
    "\n",
    "def resolve_path(pattern, roots):\n",
    "    for root in roots:\n",
    "        matches = glob.glob(os.path.join(root, pattern))\n",
    "        if matches:\n",
    "            matches.sort(key=lambda p: os.path.getmtime(p), reverse=True)\n",
    "            return matches[0]\n",
    "    return None\n",
    "\n",
    "# ---------- Load, normalize, POS-lemma ----------\n",
    "raw_dfs, missing = [], []\n",
    "for game, pat in GLOB_PATTERNS.items():\n",
    "    fpath = resolve_path(pat, INPUT_ROOTS)\n",
    "    if not fpath:\n",
    "        print(f\"âš ï¸ No match for {game} with pattern {pat} in {INPUT_ROOTS}\")\n",
    "        missing.append(game); continue\n",
    "\n",
    "    df = pd.read_parquet(fpath)\n",
    "    if not {'author','text'}.issubset(df.columns):\n",
    "        print(f\"âš ï¸ Required columns missing in {os.path.basename(fpath)} â€” skipping.\")\n",
    "        continue\n",
    "\n",
    "    df = df.dropna(subset=['author','text']).copy()\n",
    "    df['__norm'] = df['text'].map(lambda t: normalize(t) if isinstance(t,str) else \"\")\n",
    "    df['__raw_tokens'] = df['__norm'].map(tokenize_simple)\n",
    "    df['raw_tokens'] = df['__raw_tokens'].map(pos_lemmatize)\n",
    "    df['game'] = game\n",
    "\n",
    "    raw_dfs.append(df[['author','text','raw_tokens','game']])\n",
    "    print(f\"âœ… {game}: {len(df)} rows â€” {os.path.basename(fpath)}\")\n",
    "\n",
    "if not raw_dfs:\n",
    "    raise SystemExit(\"No valid inputs loaded.\")\n",
    "\n",
    "all_df = pd.concat(raw_dfs, ignore_index=True)\n",
    "print(\"ðŸ“Š Per-game counts:\", all_df['game'].value_counts().to_dict())\n",
    "\n",
    "# ---------- Train phrases on raw tokens ----------\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "\n",
    "bigram  = Phrases(all_df['raw_tokens'], min_count=BIGRAM_MIN_COUNT, threshold=PHRASE_THRESHOLD)\n",
    "trigram = Phrases(bigram[all_df['raw_tokens']], threshold=PHRASE_THRESHOLD)\n",
    "bigram_phraser  = Phraser(bigram)\n",
    "trigram_phraser = Phraser(trigram)\n",
    "\n",
    "def apply_phrases_then_filter(toks):\n",
    "    phr = trigram_phraser[bigram_phraser[toks]]\n",
    "    phr = [w for w in phr if not BAD_PHRASE.match(w)]\n",
    "    phr = [LEMMA_FIX.get(w, w) for w in phr]\n",
    "    return [w for w in phr if w not in STOP_WORDS and len(w) > 2]\n",
    "\n",
    "all_df['tokens'] = all_df['raw_tokens'].apply(apply_phrases_then_filter)\n",
    "\n",
    "# ---------- Row-level min-length filter ----------\n",
    "initial = len(all_df)\n",
    "all_df = all_df[all_df['tokens'].str.len() >= MIN_TOKENS_ROW]\n",
    "print(f\"âœ… Removed {initial - len(all_df)} short comments (<{MIN_TOKENS_ROW} tokens).\")\n",
    "\n",
    "# ---------- Peek tokens ----------\n",
    "all_tokens = [w for toks in all_df['tokens'] for w in toks]\n",
    "print(\"ðŸ”¹ Top 50 tokens:\", Counter(all_tokens).most_common(50))\n",
    "\n",
    "# ---------- Save cleaned ----------\n",
    "clean_path = os.path.join(OUT_DIR, \"Filtered_Combined_AllGames_Cleaned.parquet\")\n",
    "all_df.to_parquet(clean_path, index=False)\n",
    "print(f\"ðŸ’¾ Saved cleaned data -> {clean_path}\")\n",
    "\n",
    "# ---------- Dictionary / Corpus (with pruning) ----------\n",
    "from gensim.corpora import Dictionary\n",
    "dictionary = Dictionary(all_df['tokens'])\n",
    "dictionary.filter_extremes(no_below=NO_BELOW, no_above=NO_ABOVE, keep_n=KEEP_N)\n",
    "corpus = [dictionary.doc2bow(t) for t in all_df['tokens']]\n",
    "print(f\"ðŸ“š Dictionary: {len(dictionary)} tokens | Corpus docs: {len(corpus)}\")\n",
    "\n",
    "dict_path = os.path.join(OUT_DIR, \"lda_dictionary_AllGames.dict\")\n",
    "dictionary.save(dict_path)\n",
    "\n",
    "# Save phrasers & corpus\n",
    "bigram_phraser.save(os.path.join(OUT_DIR, \"bigram_AllGames.pkl\"))\n",
    "trigram_phraser.save(os.path.join(OUT_DIR, \"trigram_AllGames.pkl\"))\n",
    "with open(os.path.join(OUT_DIR, \"lda_corpus_AllGames.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(corpus, f)\n",
    "\n",
    "# Save metadata\n",
    "with open(os.path.join(OUT_DIR, \"cleaning_meta_AllGames.json\"), \"w\") as f:\n",
    "    json.dump({\n",
    "        \"no_below\": NO_BELOW,\n",
    "        \"no_above\": NO_ABOVE,\n",
    "        \"keep_n\": KEEP_N,\n",
    "        \"bigram_min_count\": BIGRAM_MIN_COUNT,\n",
    "        \"phrase_threshold\": PHRASE_THRESHOLD,\n",
    "        \"min_tokens_row\": MIN_TOKENS_ROW,\n",
    "        \"keep_franchise_tokens\": KEEP_FRANCHISE_TOKENS,\n",
    "        \"stopwords_sizes\": {\"nltk\": len(NLTK_STOP), \"custom\": len(CUSTOM_STOP)},\n",
    "    }, f, indent=2)\n",
    "\n",
    "# ---------- Stratified 90/10 split BY GAME for later K-sweeps ----------\n",
    "rng_state = 11\n",
    "by_game = defaultdict(list)\n",
    "for i, g in enumerate(all_df['game']):\n",
    "    by_game[g].append(i)\n",
    "\n",
    "hold_idx = set()\n",
    "for g, idxs in by_game.items():\n",
    "    r = random.Random(rng_state)\n",
    "    r.shuffle(idxs)\n",
    "    k = max(1, int(0.10 * len(idxs)))\n",
    "    hold_idx.update(idxs[:k])\n",
    "\n",
    "train_idx = [i for i in range(len(all_df)) if i not in hold_idx]\n",
    "test_idx  = [i for i in range(len(all_df)) if i in hold_idx]\n",
    "split_path = os.path.join(OUT_DIR, \"lda_split_AllGames_stratified.json\")\n",
    "with open(split_path, \"w\") as f:\n",
    "    json.dump({\"random_state\": rng_state, \"train_idx\": train_idx, \"test_idx\": test_idx}, f, indent=2)\n",
    "\n",
    "print(f\"ðŸ§ª Stratified split saved â€” Train: {len(train_idx)}  Test: {len(test_idx)}\")\n",
    "print(f\"âœ… Artifacts saved:\\n- Cleaned: {clean_path}\\n- Dictionary: {dict_path}\\n- Split: {split_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Loading data/dictionary...\n",
      "âœ… docs=423032  vocab=40194\n",
      "ðŸ” Using existing split: C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\final\\lda_split_AllGames_stratified.json\n",
      "ðŸ§ª Stratified split â€” Train: 380734  Test: 42298\n",
      "\n",
      "â³ Training LDA (k=1) ...\n",
      "ðŸ“ˆ k=1 | c_v=0.4697 | log_perplexity=-8.7321 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=2) ...\n",
      "ðŸ“ˆ k=2 | c_v=0.4547 | log_perplexity=-8.9449 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=3) ...\n",
      "ðŸ“ˆ k=3 | c_v=0.4354 | log_perplexity=-9.0958 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=4) ...\n",
      "ðŸ“ˆ k=4 | c_v=0.4248 | log_perplexity=-9.2249 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=5) ...\n",
      "ðŸ“ˆ k=5 | c_v=0.4270 | log_perplexity=-9.3278 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=6) ...\n",
      "ðŸ“ˆ k=6 | c_v=0.4316 | log_perplexity=-9.3860 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=7) ...\n",
      "ðŸ“ˆ k=7 | c_v=0.4671 | log_perplexity=-9.4704 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=8) ...\n",
      "ðŸ“ˆ k=8 | c_v=0.4593 | log_perplexity=-9.6207 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=9) ...\n",
      "ðŸ“ˆ k=9 | c_v=0.4628 | log_perplexity=-9.8243 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=10) ...\n",
      "ðŸ“ˆ k=10 | c_v=0.4430 | log_perplexity=-10.1071 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=11) ...\n",
      "ðŸ“ˆ k=11 | c_v=0.4318 | log_perplexity=-10.3794 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=12) ...\n",
      "ðŸ“ˆ k=12 | c_v=0.3977 | log_perplexity=-10.7535 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=13) ...\n",
      "ðŸ“ˆ k=13 | c_v=0.4000 | log_perplexity=-11.0513 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=14) ...\n",
      "ðŸ“ˆ k=14 | c_v=0.4258 | log_perplexity=-11.3178 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=15) ...\n",
      "ðŸ“ˆ k=15 | c_v=0.4278 | log_perplexity=-11.5718 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=16) ...\n",
      "ðŸ“ˆ k=16 | c_v=0.4012 | log_perplexity=-11.7141 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=17) ...\n",
      "ðŸ“ˆ k=17 | c_v=0.3828 | log_perplexity=-11.8640 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=18) ...\n",
      "ðŸ“ˆ k=18 | c_v=0.3895 | log_perplexity=-12.0031 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=19) ...\n",
      "ðŸ“ˆ k=19 | c_v=0.3940 | log_perplexity=-12.1539 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=20) ...\n",
      "ðŸ“ˆ k=20 | c_v=0.3772 | log_perplexity=-12.3021 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=21) ...\n",
      "ðŸ“ˆ k=21 | c_v=0.3672 | log_perplexity=-12.4360 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=22) ...\n",
      "ðŸ“ˆ k=22 | c_v=0.3563 | log_perplexity=-12.5631 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=23) ...\n",
      "ðŸ“ˆ k=23 | c_v=0.3966 | log_perplexity=-12.6904 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=24) ...\n",
      "ðŸ“ˆ k=24 | c_v=0.3607 | log_perplexity=-12.8434 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=25) ...\n",
      "ðŸ“ˆ k=25 | c_v=0.3423 | log_perplexity=-12.9844 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=26) ...\n",
      "ðŸ“ˆ k=26 | c_v=0.3733 | log_perplexity=-13.1165 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=27) ...\n",
      "ðŸ“ˆ k=27 | c_v=0.3708 | log_perplexity=-13.2536 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=28) ...\n",
      "ðŸ“ˆ k=28 | c_v=0.3528 | log_perplexity=-13.3921 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=29) ...\n",
      "ðŸ“ˆ k=29 | c_v=0.3603 | log_perplexity=-13.5273 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=30) ...\n",
      "ðŸ“ˆ k=30 | c_v=0.3477 | log_perplexity=-13.6771 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=31) ...\n",
      "ðŸ“ˆ k=31 | c_v=0.3510 | log_perplexity=-13.7857 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=32) ...\n",
      "ðŸ“ˆ k=32 | c_v=0.3605 | log_perplexity=-13.9189 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=33) ...\n",
      "ðŸ“ˆ k=33 | c_v=0.3391 | log_perplexity=-14.0493 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=34) ...\n",
      "ðŸ“ˆ k=34 | c_v=0.3425 | log_perplexity=-14.1969 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=35) ...\n",
      "ðŸ“ˆ k=35 | c_v=0.3394 | log_perplexity=-14.3013 (higher = better)\n",
      "\n",
      "ðŸ“ Saved metrics -> C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\final\\lda_k_selection_AllGames_metrics_1_35.csv\n",
      "ðŸ† Best K=1 | c_v=0.4697 | log_perplexity=-8.7321\n",
      "ðŸ’¾ Saved best model -> C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\final\\best_lda_model_AllGames_k1.model\n",
      "ðŸ—‚ï¸ Topic top-terms saved -> C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\final\\best_topics_AllGames_k1.csv\n",
      "ðŸ–¼ï¸ Saved combined plot -> C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\final\\lda_k_sweep_AllGames_1_35.png\n",
      "ðŸ–¼ï¸ Saved plot -> C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\final\\lda_k_sweep_AllGames_coherence_only.png\n",
      "ðŸ–¼ï¸ Saved plot -> C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\final\\lda_k_sweep_AllGames_logperp_only.png\n"
     ]
    }
   ],
   "source": [
    "# === LDA K sweep (ALL GAMES, K = 1..35) â€” stratified split, CSV, and plots ===\n",
    "import os, math, json, random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# ---- Config ----\n",
    "OUT_DIR         = r\"C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\final\"\n",
    "INPUT_FILE      = os.path.join(OUT_DIR, \"Filtered_Combined_AllGames_Cleaned.parquet\")\n",
    "DICT_PATH       = os.path.join(OUT_DIR, \"lda_dictionary_AllGames.dict\")\n",
    "\n",
    "K_GRID          = list(range(1, 36))  # 1..35 inclusive\n",
    "RANDOM_STATE    = 11\n",
    "PASSES, ITERS   = 5, 400\n",
    "CHUNKSIZE       = 2000\n",
    "WORKERS         = max(1, (os.cpu_count() or 1))\n",
    "\n",
    "RESULTS_CSV     = os.path.join(OUT_DIR, \"lda_k_selection_AllGames_metrics_1_35.csv\")\n",
    "SPLIT_JSON      = os.path.join(OUT_DIR, \"lda_split_AllGames_stratified.json\")\n",
    "PLOT_COMBINED   = os.path.join(OUT_DIR, \"lda_k_sweep_AllGames_1_35.png\")\n",
    "PLOT_COH_ONLY   = os.path.join(OUT_DIR, \"lda_k_sweep_AllGames_coherence_only.png\")\n",
    "PLOT_LP_ONLY    = os.path.join(OUT_DIR, \"lda_k_sweep_AllGames_logperp_only.png\")\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ---- Load ----\n",
    "print(\"ðŸ“‚ Loading data/dictionary...\")\n",
    "df = pd.read_parquet(INPUT_FILE)\n",
    "texts = df[\"tokens\"].tolist()\n",
    "games = df[\"game\"].tolist() if \"game\" in df.columns else [\"ALL\"] * len(texts)\n",
    "\n",
    "dictionary = Dictionary.load(DICT_PATH)\n",
    "corpus = [dictionary.doc2bow(t) for t in texts]\n",
    "print(f\"âœ… docs={len(corpus)}  vocab={len(dictionary)}\")\n",
    "\n",
    "# ---- Stratified train/test split by game (90/10) ----\n",
    "if os.path.exists(SPLIT_JSON):\n",
    "    print(f\"ðŸ” Using existing split: {SPLIT_JSON}\")\n",
    "    with open(SPLIT_JSON, \"r\") as f:\n",
    "        split = json.load(f)\n",
    "    train_idx, test_idx = split[\"train_idx\"], split[\"test_idx\"]\n",
    "else:\n",
    "    print(\"ðŸ†• Creating stratified split (90/10 by game)...\")\n",
    "    rng = random.Random(RANDOM_STATE)\n",
    "    by_game = defaultdict(list)\n",
    "    for i, g in enumerate(games):\n",
    "        by_game[g].append(i)\n",
    "\n",
    "    hold_idx = set()\n",
    "    for g, idxs in by_game.items():\n",
    "        rng.shuffle(idxs)\n",
    "        k = max(1, int(0.10 * len(idxs)))\n",
    "        hold_idx.update(idxs[:k])\n",
    "\n",
    "    train_idx = [i for i in range(len(corpus)) if i not in hold_idx]\n",
    "    test_idx  = [i for i in range(len(corpus)) if i in hold_idx]\n",
    "\n",
    "    with open(SPLIT_JSON, \"w\") as f:\n",
    "        json.dump({\"random_state\": RANDOM_STATE, \"train_idx\": train_idx, \"test_idx\": test_idx}, f)\n",
    "\n",
    "train_corpus = [corpus[i] for i in train_idx]\n",
    "test_corpus  = [corpus[i] for i in test_idx]\n",
    "train_texts  = [texts[i] for i in train_idx]\n",
    "print(f\"ðŸ§ª Stratified split â€” Train: {len(train_corpus)}  Test: {len(test_corpus)}\")\n",
    "\n",
    "# ---- Train/eval helper ----\n",
    "def train_eval(k: int):\n",
    "    model = LdaMulticore(\n",
    "        corpus=train_corpus,\n",
    "        id2word=dictionary,\n",
    "        num_topics=k,\n",
    "        passes=PASSES,\n",
    "        iterations=ITERS,\n",
    "        random_state=RANDOM_STATE,\n",
    "        workers=WORKERS,\n",
    "        chunksize=CHUNKSIZE,\n",
    "        eval_every=None,\n",
    "        alpha='asymmetric',\n",
    "        eta='auto',\n",
    "    )\n",
    "    # Coherence on TRAIN to avoid leakage\n",
    "    c_v = CoherenceModel(model=model, texts=train_texts, dictionary=dictionary, coherence=\"c_v\").get_coherence()\n",
    "    # Held-out log_perplexity on TEST: higher (less negative) is better\n",
    "    log_perp = model.log_perplexity(test_corpus)\n",
    "    return model, c_v, log_perp\n",
    "\n",
    "# ---- Sweep K ----\n",
    "rows = []\n",
    "best = {\"k\": None, \"c_v\": -math.inf, \"log_perplexity\": -math.inf, \"model\": None}\n",
    "\n",
    "for k in K_GRID:\n",
    "    print(f\"\\nâ³ Training LDA (k={k}) ...\")\n",
    "    model, c_v, log_perp = train_eval(k)\n",
    "    print(f\"ðŸ“ˆ k={k} | c_v={c_v:.4f} | log_perplexity={log_perp:.4f} (higher = better)\")\n",
    "    rows.append({\"k\": k, \"c_v\": c_v, \"log_perplexity\": log_perp})\n",
    "\n",
    "    # Best by highest c_v; tie-break by highest log_perplexity\n",
    "    if (c_v > best[\"c_v\"]) or (math.isclose(c_v, best[\"c_v\"], rel_tol=1e-6) and log_perp > best[\"log_perplexity\"]):\n",
    "        best.update({\"k\": k, \"c_v\": c_v, \"log_perplexity\": log_perp, \"model\": model})\n",
    "\n",
    "# ---- Save metrics table ----\n",
    "dfm = pd.DataFrame(rows).sort_values(\"k\")\n",
    "dfm.to_csv(RESULTS_CSV, index=False)\n",
    "print(f\"\\nðŸ“ Saved metrics -> {RESULTS_CSV}\")\n",
    "\n",
    "# ---- Save best model & topic terms ----\n",
    "best_k = best[\"k\"]\n",
    "best_model = best[\"model\"]\n",
    "best_path = os.path.join(OUT_DIR, f\"best_lda_model_AllGames_k{best_k}.model\")\n",
    "best_model.save(best_path)\n",
    "print(f\"ðŸ† Best K={best_k} | c_v={best['c_v']:.4f} | log_perplexity={best['log_perplexity']:.4f}\")\n",
    "print(f\"ðŸ’¾ Saved best model -> {best_path}\")\n",
    "\n",
    "def dump_topics(model, topn=20, path=None):\n",
    "    rows = []\n",
    "    for t in range(model.num_topics):\n",
    "        for rank, (w, p) in enumerate(model.show_topic(t, topn=topn), start=1):\n",
    "            rows.append({\"topic\": t, \"rank\": rank, \"word\": w, \"prob\": p})\n",
    "    dt = pd.DataFrame(rows)\n",
    "    if path: dt.to_csv(path, index=False)\n",
    "    return dt\n",
    "\n",
    "topics_csv = os.path.join(OUT_DIR, f\"best_topics_AllGames_k{best_k}.csv\")\n",
    "dump_topics(best_model, topn=20, path=topics_csv)\n",
    "print(f\"ðŸ—‚ï¸ Topic top-terms saved -> {topics_csv}\")\n",
    "\n",
    "# ---- Plots ----\n",
    "def plot_combined(df, best_k, out_path):\n",
    "    df = df.sort_values(\"k\")\n",
    "    fig, ax1 = plt.subplots(figsize=(9, 5))\n",
    "    ax1.plot(df[\"k\"], df[\"c_v\"], marker=\"o\", label=\"c_v\")\n",
    "    ax1.set_xlabel(\"K (number of topics)\")\n",
    "    ax1.set_ylabel(\"Coherence (c_v)\")\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(df[\"k\"], df[\"log_perplexity\"], marker=\"s\", linestyle=\"--\", label=\"log_perplexity\")\n",
    "    ax2.set_ylabel(\"log_perplexity (higher is better)\")\n",
    "\n",
    "    ax1.axvline(best_k, linestyle=\":\", linewidth=1.5)\n",
    "    ax1.set_title(f\"LDA K Sweep (All Games, K=1..35) â€” Best K={best_k}\")\n",
    "\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc=\"best\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"ðŸ–¼ï¸ Saved combined plot -> {out_path}\")\n",
    "\n",
    "def plot_single(x, y, ylabel, title, out_path, marker=\"o\"):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(x, y, marker=marker)\n",
    "    plt.xlabel(\"K (number of topics)\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"ðŸ–¼ï¸ Saved plot -> {out_path}\")\n",
    "\n",
    "# Combined (twin-axis)\n",
    "plot_combined(dfm, best_k, PLOT_COMBINED)\n",
    "\n",
    "# Separate per-metric plots\n",
    "plot_single(dfm[\"k\"], dfm[\"c_v\"], \"Coherence (c_v)\", \"LDA K Sweep â€” Coherence (All Games)\", PLOT_COH_ONLY, marker=\"o\")\n",
    "plot_single(dfm[\"k\"], dfm[\"log_perplexity\"], \"log_perplexity (higher is better)\", \"LDA K Sweep â€” log_perplexity (All Games)\", PLOT_LP_ONLY, marker=\"s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# ðŸ”¹ Step 1: Load Cleaned Tokenized Data\n",
    "lda_data_path = r\"C:/Users/colin/Box/2024-colin-viktor/Videogame Scraping Project/data/final/Filtered_Combined_AllGames_Cleaned.parquet\"\n",
    "df = pd.read_parquet(lda_data_path)\n",
    "\n",
    "# ðŸ”¹ Step 2: Create Dictionary & Corpus\n",
    "dictionary = Dictionary(df[\"tokens\"])\n",
    "corpus = [dictionary.doc2bow(tokens) for tokens in df[\"tokens\"]]\n",
    "print(f\"âœ… Dictionary and corpus created with {len(dictionary)} unique tokens.\")\n",
    "\n",
    "# ðŸ”¹ Step 3: Define Function to Print Top Words for Each Topic\n",
    "def print_top_words(lda_model, topn=10):\n",
    "    k = lda_model.num_topics\n",
    "    print(f\"\\nðŸ”¹ Top {topn} Words per Topic (K = {k})\")\n",
    "    for topic_id in range(k):\n",
    "        words = [word for word, _ in lda_model.show_topic(topic_id, topn=topn)]\n",
    "        print(f\"Topic {topic_id + 1}: {', '.join(words)}\")\n",
    "\n",
    "# ðŸ”¹ Step 4: Run LDA for 4 Topics\n",
    "print(\"\\nðŸ”¹ Running LDA for 4 Topics...\")\n",
    "lda_model_4 = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=4,\n",
    "    iterations=5000,\n",
    "    random_state=42\n",
    ")\n",
    "print_top_words(lda_model_4, topn=10)\n",
    "\n",
    "# ðŸ”¹ Step 5: Run LDA for 5 Topics\n",
    "print(\"\\nðŸ”¹ Running LDA for 5 Topics...\")\n",
    "lda_model_5 = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=5,\n",
    "    iterations=5000,\n",
    "    random_state=42\n",
    ")\n",
    "print_top_words(lda_model_5, topn=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
