{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\colin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: nltk in c:\\users\\colin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: gensim in c:\\users\\colin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\colin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (21.0.0)\n",
      "Requirement already satisfied: fastparquet in c:\\users\\colin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2024.11.0)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.7-cp311-cp311-win_amd64.whl (8.1 MB)\n",
      "     ---------------------------------------- 8.1/8.1 MB 8.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\colin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\colin\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\colin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\colin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: click in c:\\users\\colin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\colin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\colin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (2025.9.18)\n",
      "Requirement already satisfied: tqdm in c:\\users\\colin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\colin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\colin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim) (7.3.1)\n",
      "Requirement already satisfied: cramjam>=2.3 in c:\\users\\colin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fastparquet) (2.11.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\colin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fastparquet) (2025.9.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\colin\\appdata\\roaming\\python\\python311\\site-packages (from fastparquet) (25.0)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.3.3-cp311-cp311-win_amd64.whl (225 kB)\n",
      "     ---------------------------------------- 225.2/225.2 kB ? eta 0:00:00\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.60.1-cp311-cp311-win_amd64.whl (2.3 MB)\n",
      "     ---------------------------------------- 2.3/2.3 MB 16.2 MB/s eta 0:00:00\n",
      "Collecting kiwisolver>=1.3.1\n",
      "  Downloading kiwisolver-1.4.9-cp311-cp311-win_amd64.whl (73 kB)\n",
      "     ---------------------------------------- 73.8/73.8 kB 1.4 MB/s eta 0:00:00\n",
      "Collecting pillow>=8\n",
      "  Downloading pillow-11.3.0-cp311-cp311-win_amd64.whl (7.0 MB)\n",
      "     ---------------------------------------- 7.0/7.0 MB 8.0 MB/s eta 0:00:00\n",
      "Collecting pyparsing>=3\n",
      "  Downloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "     -------------------------------------- 113.9/113.9 kB 6.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\colin\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\colin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\colin\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk) (0.4.6)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.60.1 kiwisolver-1.4.9 matplotlib-3.10.7 pillow-11.3.0 pyparsing-3.2.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts fonttools.exe, pyftmerge.exe, pyftsubset.exe and ttx.exe are installed in 'c:\\Users\\Colin\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pandas nltk gensim pyarrow fastparquet matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Colin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Legend of Zelda: 186688 rows â€” Legend_of_Zelda_Breath_of_the_Wild_Comments_Analysis.parquet\n",
      "âœ… Baldur's Gate 3: 10572 rows â€” Baldur's_Gate_3_Comments_Analysis.parquet\n",
      "âœ… Elden Ring: 121397 rows â€” Elden_Ring_Comments_Analysis.parquet\n",
      "âœ… Hollow Knight: 52835 rows â€” Hollow_Knight_Comments_Analysis.parquet\n",
      "âœ… Red Dead Redemption 2: 159479 rows â€” Red_Dead_Redemption_2_Comments_Analysis.parquet\n",
      "ðŸ“Š Per-game counts: {'Legend of Zelda': 186688, 'Red Dead Redemption 2': 159479, 'Elden Ring': 121397, 'Hollow Knight': 52835, \"Baldur's Gate 3\": 10572}\n",
      "âœ… Removed 329978 short comments (<5 tokens).\n",
      "ðŸ”¹ Top 50 tokens: [('kill', 14777), ('find', 14548), ('try', 13541), ('fight', 11191), ('end', 10251), ('boss', 8920), ('die', 8339), ('arthur', 7996), ('hit', 7231), ('horse', 6519), ('miss', 6259), ('beat', 6206), ('hard', 5841), ('tell', 5501), ('mission', 5209), ('run', 5190), ('weapon', 4860), ('easy', 4800), ('john', 4796), ('gonna', 4717), ('call', 4623), ('attack', 4527), ('leave', 4525), ('big', 4357), ('enemy', 4310), ('finish', 4252), ('link', 4055), ('remember', 4037), ('far', 3990), ('zelda', 3961), ('hour', 3932), ('stop', 3917), ('cowboy', 3869), ('character', 3780), ('story', 3749), ('happen', 3726), ('enjoy', 3706), ('seem', 3651), ('probably', 3643), ('whole', 3518), ('yet', 3503), ('hear', 3469), ('anyone', 3459), ('follow', 3395), ('might', 3369), ('instead', 3340), ('world', 3326), ('dutch', 3319), ('old', 3278), ('jump', 3169)]\n",
      "ðŸ’¾ Saved cleaned data -> C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\final\\Filtered_Combined_SD_Cleaned.parquet\n",
      "ðŸ“š Dictionary: 27106 tokens | Corpus docs: 200993\n"
     ]
    }
   ],
   "source": [
    "# ========= POS-aware Cleaning Pipeline (NLTK) with lemma fixes =========\n",
    "import os, re, glob, json\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "# ---------- Ensure NLTK resources ----------\n",
    "for pkg in (\"stopwords\", \"punkt\", \"wordnet\"):\n",
    "    try:\n",
    "        nltk.data.find(f\"corpora/{pkg}\" if pkg != \"punkt\" else \"tokenizers/punkt\")\n",
    "    except LookupError:\n",
    "        nltk.download(pkg)\n",
    "\n",
    "# Tagger (try new name first, fallback to old one)\n",
    "try:\n",
    "    nltk.data.find(\"taggers/averaged_perceptron_tagger_eng\")\n",
    "except LookupError:\n",
    "    try:\n",
    "        nltk.download(\"averaged_perceptron_tagger_eng\")\n",
    "    except:\n",
    "        nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "from nltk.corpus import stopwords, wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag_sents  # batch POS tagging for speed\n",
    "\n",
    "# ---------- Config ----------\n",
    "INPUT_ROOTS = [\n",
    "    r\"C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\raw\",\n",
    "]\n",
    "OUTPUT_DIR  = r\"C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\final\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "GLOB_PATTERNS = {\n",
    "    \"Legend of Zelda\":        \"Legend*Wild*Comments*Analysis.parquet\",\n",
    "    \"Baldur's Gate 3\":        \"Baldur*Gate*3*Comments*Analysis.parquet\",\n",
    "    \"Elden Ring\":             \"Elden*Ring*Comments*Analysis.parquet\",\n",
    "    \"Hollow Knight\":          \"Hollow*Knight*Comments*Analysis.parquet\",\n",
    "    \"Red Dead Redemption 2\":  \"Red*Dead*Redemption*2*Comments*Analysis.parquet\",\n",
    "}\n",
    "\n",
    "# Phrase thresholds\n",
    "BIGRAM_MIN_COUNT = 5\n",
    "PHRASE_THRESHOLD = 8.0\n",
    "MIN_TOKENS_ROW   = 5\n",
    "\n",
    "# Dictionary pruning\n",
    "NO_BELOW = 5\n",
    "NO_ABOVE = 0.5\n",
    "KEEP_N   = 100_000\n",
    "\n",
    "# ---------- Stopwords ----------\n",
    "NLTK_STOP = set(stopwords.words(\"english\"))\n",
    "CUSTOM_STOP = {\n",
    "    'video','game','online','youtube','series','pls','lol','omg','xd','people','thing',\n",
    "    'play','make','time','love','look','want','think','watch','know','got','use','cant',\n",
    "    'going','never','ever','part','help','played','getting','doesnt','bad','pretty',\n",
    "    'show','fuck','talk','went','comment','cool','amazing','seen','best','like','get','one',\n",
    "    'dont','would','first','really','see','also','dan','way','guy','good','say','back',\n",
    "    'much','still','even','man','thats','need','bro','new','kid','every','always','could',\n",
    "    'said','please','youre','actually','didnt','feel','ive','dude','name',\n",
    "    'keep','gon','watching','everyone','hey','someone','made','come','great',\n",
    "    'give','well','fun','nice','let','right','day','friend','thought','work','mean','take',\n",
    "    'vid','lmao','lot','god','something','hope','put','cause','literally','since','next','hate',\n",
    "    'used','saying','funny','many','vids','episode','playthrough','playing','thank','thanks','sure',\n",
    "    'two','though','last','stuff','without','everything','maybe','second','around','long','place',\n",
    "    'point','already','year','little','another','better','fucking','shit','area','found','wait','merg',\n",
    "    'wouldnt','wouldve','youve','youll','wasnt','aint','couldnt','seems','happens','happened','taking',\n",
    "    'honestly','definitely','either','looking','looked','open','add','top','full','mine','kept','tried','gave','minute','damn',\n",
    "    'channel','walkthrough','content','using','done','start'\n",
    "}\n",
    "CREATOR_NAMES = {'arin','jack','brad','delirious','theradbrad','gamegrumps'}\n",
    "STOP_WORDS = NLTK_STOP.union(CUSTOM_STOP).union(CREATOR_NAMES)\n",
    "\n",
    "# ---------- Regex & helpers ----------\n",
    "URL_RE   = re.compile(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\")\n",
    "HTML_RE  = re.compile(r\"<.*?>\")\n",
    "PUNC_RE  = re.compile(r\"[^\\w\\s]\")\n",
    "DIGIT_RE = re.compile(r\"\\d+\")\n",
    "WS_RE    = re.compile(r\"\\s+\")\n",
    "\n",
    "LEMM = WordNetLemmatizer()\n",
    "\n",
    "# Fix lemmatization quirks (e.g., 'boss' -> 'bos')\n",
    "LEMMA_FIX = {\n",
    "    \"bos\": \"boss\",\n",
    "    # add more if spotted later\n",
    "}\n",
    "\n",
    "def _wn_pos(tag: str):\n",
    "    \"\"\"Map Penn POS to WordNet POS.\"\"\"\n",
    "    if not tag:\n",
    "        return wn.NOUN\n",
    "    t = tag[0]\n",
    "    if t == 'J': return wn.ADJ\n",
    "    if t == 'V': return wn.VERB\n",
    "    if t == 'N': return wn.NOUN\n",
    "    if t == 'R': return wn.ADV\n",
    "    return wn.NOUN\n",
    "\n",
    "def normalize(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = URL_RE.sub(\" \", text)\n",
    "    text = HTML_RE.sub(\" \", text)\n",
    "    text = PUNC_RE.sub(\" \", text)\n",
    "    text = DIGIT_RE.sub(\" \", text)\n",
    "    text = WS_RE.sub(\" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def tokenize_simple(text: str):\n",
    "    return text.split()\n",
    "\n",
    "def pos_lemmatize_tokens(tokens):\n",
    "    tagged = list(pos_tag_sents([tokens]))[0]\n",
    "    return [LEMM.lemmatize(w, _wn_pos(tag)) for (w, tag) in tagged]\n",
    "\n",
    "def apply_lemma_fixes(tokens):\n",
    "    return [LEMMA_FIX.get(t, t) for t in tokens]\n",
    "\n",
    "def clean_text_pos(text: str):\n",
    "    \"\"\"Full cleaner with POS-aware lemmatization + lemma fixes.\"\"\"\n",
    "    if not isinstance(text, str) or not text:\n",
    "        return []\n",
    "    text = normalize(text)\n",
    "    toks = tokenize_simple(text)\n",
    "    toks = [t for t in toks if len(t) > 2]\n",
    "    if not toks:\n",
    "        return []\n",
    "    toks = pos_lemmatize_tokens(toks)        # POS-aware lemmatization\n",
    "    toks = apply_lemma_fixes(toks)           # fix known quirks (boss/bos)\n",
    "    toks = [t for t in toks if t not in STOP_WORDS and len(t) > 2]\n",
    "    return toks\n",
    "\n",
    "# ---------- Robust file resolver ----------\n",
    "def resolve_path(pattern, roots):\n",
    "    for root in roots:\n",
    "        matches = glob.glob(os.path.join(root, pattern))\n",
    "        if matches:\n",
    "            matches.sort(key=lambda p: os.path.getmtime(p), reverse=True)\n",
    "            return matches[0]\n",
    "    return None\n",
    "\n",
    "# ---------- Load, clean, combine ----------\n",
    "cleaned_dfs, missing = [], []\n",
    "for game, pat in GLOB_PATTERNS.items():\n",
    "    fpath = resolve_path(pat, INPUT_ROOTS)\n",
    "    if not fpath:\n",
    "        print(f\"âš ï¸ No match for {game} with pattern {pat} in {INPUT_ROOTS}\")\n",
    "        missing.append(game)\n",
    "        continue\n",
    "    df = pd.read_parquet(fpath)\n",
    "    if not {'author','text'}.issubset(df.columns):\n",
    "        print(f\"âš ï¸ Required columns missing in {os.path.basename(fpath)} â€” skipping.\")\n",
    "        continue\n",
    "    df = df.dropna(subset=['author','text']).copy()\n",
    "    df['tokens'] = df['text'].map(clean_text_pos)\n",
    "    df = df[df['tokens'].str.len() > 0].drop_duplicates(subset=['text'])\n",
    "    df['game'] = game\n",
    "    cleaned_dfs.append(df)\n",
    "    print(f\"âœ… {game}: {len(df)} rows â€” {os.path.basename(fpath)}\")\n",
    "\n",
    "if not cleaned_dfs:\n",
    "    raise SystemExit(\"No valid input files loaded.\")\n",
    "\n",
    "story_comments = pd.concat(cleaned_dfs, ignore_index=True)\n",
    "print(\"ðŸ“Š Per-game counts:\", story_comments['game'].value_counts().to_dict())\n",
    "\n",
    "# ---------- Phrase modeling (then refilter stopwords & reapply lemma fixes) ----------\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "\n",
    "bigram  = Phrases(story_comments['tokens'], min_count=BIGRAM_MIN_COUNT, threshold=PHRASE_THRESHOLD)\n",
    "trigram = Phrases(bigram[story_comments['tokens']], threshold=PHRASE_THRESHOLD)\n",
    "bigram_phraser  = Phraser(bigram)\n",
    "trigram_phraser = Phraser(trigram)\n",
    "\n",
    "def apply_phrases_and_refilter(toks):\n",
    "    phrased = trigram_phraser[bigram_phraser[toks]]\n",
    "    phrased = apply_lemma_fixes(phrased)  # catch any phrase-stage quirks\n",
    "    return [w for w in phrased if w not in STOP_WORDS and len(w) > 2]\n",
    "\n",
    "story_comments['tokens'] = story_comments['tokens'].apply(apply_phrases_and_refilter)\n",
    "\n",
    "# ---------- Row-level min-length filter ----------\n",
    "initial = len(story_comments)\n",
    "story_comments = story_comments[story_comments['tokens'].str.len() >= MIN_TOKENS_ROW]\n",
    "print(f\"âœ… Removed {initial - len(story_comments)} short comments (<{MIN_TOKENS_ROW} tokens).\")\n",
    "\n",
    "# ---------- Token peek ----------\n",
    "all_tokens = [w for toks in story_comments['tokens'] for w in toks]\n",
    "print(\"ðŸ”¹ Top 50 tokens:\", Counter(all_tokens).most_common(50))\n",
    "\n",
    "# ---------- Save cleaned ----------\n",
    "clean_path = os.path.join(OUTPUT_DIR, \"Filtered_Combined_SD_Cleaned.parquet\")\n",
    "story_comments.to_parquet(clean_path, index=False)\n",
    "print(f\"ðŸ’¾ Saved cleaned data -> {clean_path}\")\n",
    "\n",
    "# ---------- Dictionary / Corpus (with pruning knobs) ----------\n",
    "from gensim.corpora import Dictionary\n",
    "dictionary = Dictionary(story_comments['tokens'])\n",
    "dictionary.filter_extremes(no_below=NO_BELOW, no_above=NO_ABOVE, keep_n=KEEP_N)\n",
    "corpus = [dictionary.doc2bow(t) for t in story_comments['tokens']]\n",
    "print(f\"ðŸ“š Dictionary: {len(dictionary)} tokens | Corpus docs: {len(corpus)}\")\n",
    "\n",
    "dictionary.save(os.path.join(OUTPUT_DIR, \"lda_dictionary_SD.dict\"))\n",
    "\n",
    "# Save phrasers & corpus to avoid recompute later\n",
    "bigram_phraser.save(os.path.join(OUTPUT_DIR, \"bigram_SD.pkl\"))\n",
    "trigram_phraser.save(os.path.join(OUTPUT_DIR, \"trigram_SD.pkl\"))\n",
    "import pickle\n",
    "with open(os.path.join(OUTPUT_DIR, \"lda_corpus_SD.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(corpus, f)\n",
    "\n",
    "# Save basic metadata (handy for reproducibility)\n",
    "with open(os.path.join(OUTPUT_DIR, \"cleaning_meta_SD.json\"), \"w\") as f:\n",
    "    json.dump({\n",
    "        \"no_below\": NO_BELOW,\n",
    "        \"no_above\": NO_ABOVE,\n",
    "        \"keep_n\": KEEP_N,\n",
    "        \"bigram_min_count\": BIGRAM_MIN_COUNT,\n",
    "        \"phrase_threshold\": PHRASE_THRESHOLD,\n",
    "        \"min_tokens_row\": MIN_TOKENS_ROW,\n",
    "        \"stopwords_sizes\": {\"nltk\": len(NLTK_STOP), \"custom\": len(CUSTOM_STOP), \"creators\": len(CREATOR_NAMES)},\n",
    "        \"lemma_fixes\": list(LEMMA_FIX.items()),\n",
    "    }, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Loading data/dictionary...\n",
      "âœ… docs=200993  vocab=27106\n",
      "ðŸ§ª Stratified split â€” Train: 180896  Test: 20097\n",
      "\n",
      "â³ Training LDA (k=1) ...\n",
      "ðŸ“ˆ k=1 | c_v=0.5419 | log_perplexity=-8.7592 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=2) ...\n",
      "ðŸ“ˆ k=2 | c_v=0.5194 | log_perplexity=-8.9502 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=3) ...\n",
      "ðŸ“ˆ k=3 | c_v=0.5786 | log_perplexity=-9.1263 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=4) ...\n",
      "ðŸ“ˆ k=4 | c_v=0.5060 | log_perplexity=-9.3242 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=5) ...\n",
      "ðŸ“ˆ k=5 | c_v=0.5394 | log_perplexity=-9.4104 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=6) ...\n",
      "ðŸ“ˆ k=6 | c_v=0.5449 | log_perplexity=-9.5139 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=7) ...\n",
      "ðŸ“ˆ k=7 | c_v=0.5473 | log_perplexity=-9.6289 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=8) ...\n",
      "ðŸ“ˆ k=8 | c_v=0.5068 | log_perplexity=-9.7796 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=9) ...\n",
      "ðŸ“ˆ k=9 | c_v=0.5513 | log_perplexity=-9.9101 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=10) ...\n",
      "ðŸ“ˆ k=10 | c_v=0.5475 | log_perplexity=-10.1684 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=11) ...\n",
      "ðŸ“ˆ k=11 | c_v=0.5308 | log_perplexity=-10.4682 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=12) ...\n",
      "ðŸ“ˆ k=12 | c_v=0.4959 | log_perplexity=-10.7614 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=13) ...\n",
      "ðŸ“ˆ k=13 | c_v=0.5171 | log_perplexity=-11.0670 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=14) ...\n",
      "ðŸ“ˆ k=14 | c_v=0.4876 | log_perplexity=-11.2928 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=15) ...\n",
      "ðŸ“ˆ k=15 | c_v=0.5195 | log_perplexity=-11.4237 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=16) ...\n",
      "ðŸ“ˆ k=16 | c_v=0.4814 | log_perplexity=-11.6135 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=17) ...\n",
      "ðŸ“ˆ k=17 | c_v=0.4838 | log_perplexity=-11.7416 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=18) ...\n",
      "ðŸ“ˆ k=18 | c_v=0.4937 | log_perplexity=-11.8875 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=19) ...\n",
      "ðŸ“ˆ k=19 | c_v=0.4781 | log_perplexity=-12.0255 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=20) ...\n",
      "ðŸ“ˆ k=20 | c_v=0.4610 | log_perplexity=-12.1777 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=21) ...\n",
      "ðŸ“ˆ k=21 | c_v=0.4753 | log_perplexity=-12.3050 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=22) ...\n",
      "ðŸ“ˆ k=22 | c_v=0.4713 | log_perplexity=-12.4291 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=23) ...\n",
      "ðŸ“ˆ k=23 | c_v=0.4594 | log_perplexity=-12.5514 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=24) ...\n",
      "ðŸ“ˆ k=24 | c_v=0.4519 | log_perplexity=-12.6959 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=25) ...\n",
      "ðŸ“ˆ k=25 | c_v=0.4429 | log_perplexity=-12.8231 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=26) ...\n",
      "ðŸ“ˆ k=26 | c_v=0.4406 | log_perplexity=-12.9527 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=27) ...\n",
      "ðŸ“ˆ k=27 | c_v=0.4526 | log_perplexity=-13.0983 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=28) ...\n",
      "ðŸ“ˆ k=28 | c_v=0.4336 | log_perplexity=-13.2208 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=29) ...\n",
      "ðŸ“ˆ k=29 | c_v=0.4122 | log_perplexity=-13.3425 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=30) ...\n",
      "ðŸ“ˆ k=30 | c_v=0.4200 | log_perplexity=-13.4719 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=31) ...\n",
      "ðŸ“ˆ k=31 | c_v=0.4334 | log_perplexity=-13.6065 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=32) ...\n",
      "ðŸ“ˆ k=32 | c_v=0.4088 | log_perplexity=-13.7425 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=33) ...\n",
      "ðŸ“ˆ k=33 | c_v=0.4217 | log_perplexity=-13.8414 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=34) ...\n",
      "ðŸ“ˆ k=34 | c_v=0.4121 | log_perplexity=-13.9803 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=35) ...\n",
      "ðŸ“ˆ k=35 | c_v=0.4047 | log_perplexity=-14.0842 (higher = better)\n",
      "\n",
      "ðŸ“ Saved metrics -> C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\final\\lda_k_selection_SD_metrics_1_35.csv\n",
      "ðŸ† Best K=3 | c_v=0.5786 | log_perplexity=-9.1263\n",
      "ðŸ’¾ Saved best model -> C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\final\\best_lda_model_SD_k3.model\n",
      "ðŸ—‚ï¸ Topic top-terms saved -> C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\final\\best_topics_SD_k3.csv\n",
      "ðŸ–¼ï¸ Saved combined plot -> C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\final\\lda_k_sweep_SD_1_35.png\n",
      "ðŸ–¼ï¸ Saved plot -> C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\final\\lda_k_sweep_SD_coherence_only.png\n",
      "ðŸ–¼ï¸ Saved plot -> C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\final\\lda_k_sweep_SD_logperp_only.png\n"
     ]
    }
   ],
   "source": [
    "# === LDA K sweep (K = 1..35) â€” stratified split, CSV, and plots ===\n",
    "import os, math, json, random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# ---- Config ----\n",
    "INPUT_FILE      = r\"C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\final\\Filtered_Combined_SD_Cleaned.parquet\"\n",
    "OUT_DIR         = r\"C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\final\"\n",
    "DICT_PATH       = os.path.join(OUT_DIR, \"lda_dictionary_SD.dict\")  # use the name you saved\n",
    "K_GRID          = list(range(1, 36))  # 1..35 inclusive\n",
    "RANDOM_STATE    = 11\n",
    "PASSES, ITERS   = 5, 400              # can bump later when retraining best_k\n",
    "CHUNKSIZE       = 2000\n",
    "WORKERS         = os.cpu_count()\n",
    "\n",
    "RESULTS_CSV     = os.path.join(OUT_DIR, \"lda_k_selection_SD_metrics_1_35.csv\")\n",
    "SPLIT_JSON      = os.path.join(OUT_DIR, \"lda_split_SD_stratified.json\")\n",
    "PLOT_COMBINED   = os.path.join(OUT_DIR, \"lda_k_sweep_SD_1_35.png\")\n",
    "PLOT_COH_ONLY   = os.path.join(OUT_DIR, \"lda_k_sweep_SD_coherence_only.png\")\n",
    "PLOT_LP_ONLY    = os.path.join(OUT_DIR, \"lda_k_sweep_SD_logperp_only.png\")\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ---- Load ----\n",
    "print(\"ðŸ“‚ Loading data/dictionary...\")\n",
    "df = pd.read_parquet(INPUT_FILE)\n",
    "texts = df[\"tokens\"].tolist()\n",
    "games = df[\"game\"].tolist() if \"game\" in df.columns else [\"ALL\"] * len(texts)\n",
    "\n",
    "dictionary = Dictionary.load(DICT_PATH)\n",
    "corpus = [dictionary.doc2bow(t) for t in texts]\n",
    "print(f\"âœ… docs={len(corpus)}  vocab={len(dictionary)}\")\n",
    "\n",
    "# ---- Stratified train/test split by game (90/10) ----\n",
    "rng = random.Random(RANDOM_STATE)\n",
    "by_game = defaultdict(list)\n",
    "for i, g in enumerate(games):\n",
    "    by_game[g].append(i)\n",
    "\n",
    "hold_idx = set()\n",
    "for g, idxs in by_game.items():\n",
    "    rng.shuffle(idxs)\n",
    "    k = max(1, int(0.10 * len(idxs)))  # 10% per game\n",
    "    hold_idx.update(idxs[:k])\n",
    "\n",
    "train_idx = [i for i in range(len(corpus)) if i not in hold_idx]\n",
    "test_idx  = [i for i in range(len(corpus)) if i in hold_idx]\n",
    "\n",
    "train_corpus = [corpus[i] for i in train_idx]\n",
    "test_corpus  = [corpus[i] for i in test_idx]\n",
    "train_texts  = [texts[i] for i in train_idx]\n",
    "\n",
    "with open(SPLIT_JSON, \"w\") as f:\n",
    "    json.dump({\"random_state\": RANDOM_STATE, \"train_idx\": train_idx, \"test_idx\": test_idx}, f)\n",
    "print(f\"ðŸ§ª Stratified split â€” Train: {len(train_corpus)}  Test: {len(test_corpus)}\")\n",
    "\n",
    "# ---- Train/eval helper ----\n",
    "def train_eval(k: int):\n",
    "    model = LdaMulticore(\n",
    "        corpus=train_corpus,\n",
    "        id2word=dictionary,\n",
    "        num_topics=k,\n",
    "        passes=PASSES,\n",
    "        iterations=ITERS,\n",
    "        random_state=RANDOM_STATE,\n",
    "        workers=WORKERS,\n",
    "        chunksize=CHUNKSIZE,\n",
    "        eval_every=None,\n",
    "        # Optional priors to try later:\n",
    "        # alpha='asymmetric', eta=None\n",
    "    )\n",
    "    # Coherence on TRAIN to avoid leakage\n",
    "    c_v = CoherenceModel(model=model, texts=train_texts, dictionary=dictionary, coherence=\"c_v\").get_coherence()\n",
    "    # Held-out log_perplexity: higher (less negative) is better\n",
    "    log_perp = model.log_perplexity(test_corpus)\n",
    "    return model, c_v, log_perp\n",
    "\n",
    "# ---- Sweep K ----\n",
    "rows = []\n",
    "best = {\"k\": None, \"c_v\": -math.inf, \"log_perplexity\": -math.inf, \"model\": None}\n",
    "\n",
    "for k in K_GRID:\n",
    "    print(f\"\\nâ³ Training LDA (k={k}) ...\")\n",
    "    model, c_v, log_perp = train_eval(k)\n",
    "    print(f\"ðŸ“ˆ k={k} | c_v={c_v:.4f} | log_perplexity={log_perp:.4f} (higher = better)\")\n",
    "    rows.append({\"k\": k, \"c_v\": c_v, \"log_perplexity\": log_perp})\n",
    "\n",
    "    # Best by highest c_v; tie-break by highest log_perplexity\n",
    "    if (c_v > best[\"c_v\"]) or (math.isclose(c_v, best[\"c_v\"], rel_tol=1e-6) and log_perp > best[\"log_perplexity\"]):\n",
    "        best.update({\"k\": k, \"c_v\": c_v, \"log_perplexity\": log_perp, \"model\": model})\n",
    "\n",
    "# ---- Save metrics table ----\n",
    "dfm = pd.DataFrame(rows).sort_values(\"k\")\n",
    "dfm.to_csv(RESULTS_CSV, index=False)\n",
    "print(f\"\\nðŸ“ Saved metrics -> {RESULTS_CSV}\")\n",
    "\n",
    "# ---- Save best model & topic terms ----\n",
    "best_k = best[\"k\"]\n",
    "best_model = best[\"model\"]\n",
    "best_path = os.path.join(OUT_DIR, f\"best_lda_model_SD_k{best_k}.model\")\n",
    "best_model.save(best_path)\n",
    "print(f\"ðŸ† Best K={best_k} | c_v={best['c_v']:.4f} | log_perplexity={best['log_perplexity']:.4f}\")\n",
    "print(f\"ðŸ’¾ Saved best model -> {best_path}\")\n",
    "\n",
    "def dump_topics(model, topn=20, path=None):\n",
    "    rows = []\n",
    "    for t in range(model.num_topics):\n",
    "        for rank, (w, p) in enumerate(model.show_topic(t, topn=topn), start=1):\n",
    "            rows.append({\"topic\": t, \"rank\": rank, \"word\": w, \"prob\": p})\n",
    "    dt = pd.DataFrame(rows)\n",
    "    if path: dt.to_csv(path, index=False)\n",
    "    return dt\n",
    "\n",
    "topics_csv = os.path.join(OUT_DIR, f\"best_topics_SD_k{best_k}.csv\")\n",
    "dump_topics(best_model, topn=20, path=topics_csv)\n",
    "print(f\"ðŸ—‚ï¸ Topic top-terms saved -> {topics_csv}\")\n",
    "\n",
    "# ---- Plots ----\n",
    "def plot_combined(df, best_k, out_path):\n",
    "    df = df.sort_values(\"k\")\n",
    "    fig, ax1 = plt.subplots(figsize=(9, 5))\n",
    "    ax1.plot(df[\"k\"], df[\"c_v\"], marker=\"o\", label=\"c_v\")\n",
    "    ax1.set_xlabel(\"K (number of topics)\")\n",
    "    ax1.set_ylabel(\"Coherence (c_v)\")\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(df[\"k\"], df[\"log_perplexity\"], marker=\"s\", linestyle=\"--\", label=\"log_perplexity\")\n",
    "    ax2.set_ylabel(\"log_perplexity (higher is better)\")\n",
    "\n",
    "    ax1.axvline(best_k, linestyle=\":\", linewidth=1.5)\n",
    "    ax1.set_title(f\"LDA K Sweep (K=1..35) â€” Best K={best_k}\")\n",
    "\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc=\"best\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"ðŸ–¼ï¸ Saved combined plot -> {out_path}\")\n",
    "\n",
    "def plot_single(x, y, ylabel, title, out_path, marker=\"o\"):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(x, y, marker=marker)\n",
    "    plt.xlabel(\"K (number of topics)\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"ðŸ–¼ï¸ Saved plot -> {out_path}\")\n",
    "\n",
    "# Combined (twin-axis)\n",
    "plot_combined(dfm, best_k, PLOT_COMBINED)\n",
    "\n",
    "# Separate per-metric plots\n",
    "plot_single(dfm[\"k\"], dfm[\"c_v\"], \"Coherence (c_v)\", \"LDA K Sweep â€” Coherence\", PLOT_COH_ONLY, marker=\"o\")\n",
    "plot_single(dfm[\"k\"], dfm[\"log_perplexity\"], \"log_perplexity (higher is better)\", \"LDA K Sweep â€” log_perplexity\", PLOT_LP_ONLY, marker=\"s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Loading data/dictionary...\n",
      "âœ… docs=256182  vocab=24291\n",
      "ðŸ§ª Stratified split â€” Train: 230566  Test: 25616\n",
      "\n",
      "â³ Training LDA (k=1) ...\n",
      "ðŸ“ˆ k=1 | c_v=0.3863 | log_perplexity=-8.5922 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=2) ...\n",
      "ðŸ“ˆ k=2 | c_v=0.4742 | log_perplexity=-8.7294 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=3) ...\n",
      "ðŸ“ˆ k=3 | c_v=0.4487 | log_perplexity=-8.8450 (higher = better)\n",
      "\n",
      "â³ Training LDA (k=4) ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 73\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m K_GRID:\n\u001b[32m     72\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mâ³ Training LDA (k=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) ...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     model, c_v, log_perp = \u001b[43mtrain_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mðŸ“ˆ k=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | c_v=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc_v\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | log_perplexity=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlog_perp\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (higher = better)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     75\u001b[39m     rows.append({\u001b[33m\"\u001b[39m\u001b[33mk\u001b[39m\u001b[33m\"\u001b[39m: k, \u001b[33m\"\u001b[39m\u001b[33mc_v\u001b[39m\u001b[33m\"\u001b[39m: c_v, \u001b[33m\"\u001b[39m\u001b[33mlog_perplexity\u001b[39m\u001b[33m\"\u001b[39m: log_perp})\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 63\u001b[39m, in \u001b[36mtrain_eval\u001b[39m\u001b[34m(k)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_eval\u001b[39m(k: \u001b[38;5;28mint\u001b[39m):\n\u001b[32m     51\u001b[39m     model = LdaMulticore(\n\u001b[32m     52\u001b[39m         corpus=train_corpus,\n\u001b[32m     53\u001b[39m         id2word=dictionary,\n\u001b[32m   (...)\u001b[39m\u001b[32m     61\u001b[39m         \u001b[38;5;66;03m# alpha='asymmetric', eta='auto'  # uncomment to test alt priors after first pass\u001b[39;00m\n\u001b[32m     62\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m     c_v = \u001b[43mCoherenceModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdictionary\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdictionary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoherence\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mc_v\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_coherence\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m     log_perp = model.log_perplexity(test_corpus)  \u001b[38;5;66;03m# higher (less negative) is better\u001b[39;00m\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model, c_v, log_perp\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Colin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\coherencemodel.py:614\u001b[39m, in \u001b[36mCoherenceModel.get_coherence\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    605\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_coherence\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    606\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get coherence value based on pipeline parameters.\u001b[39;00m\n\u001b[32m    607\u001b[39m \n\u001b[32m    608\u001b[39m \u001b[33;03m    Returns\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    612\u001b[39m \n\u001b[32m    613\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m614\u001b[39m     confirmed_measures = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_coherence_per_topic\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    615\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.aggregate_measures(confirmed_measures)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Colin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\coherencemodel.py:574\u001b[39m, in \u001b[36mCoherenceModel.get_coherence_per_topic\u001b[39m\u001b[34m(self, segmented_topics, with_std, with_support)\u001b[39m\n\u001b[32m    572\u001b[39m     segmented_topics = measure.seg(\u001b[38;5;28mself\u001b[39m.topics)\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._accumulator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m574\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mestimate_probabilities\u001b[49m\u001b[43m(\u001b[49m\u001b[43msegmented_topics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    576\u001b[39m kwargs = \u001b[38;5;28mdict\u001b[39m(with_std=with_std, with_support=with_support)\n\u001b[32m    577\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.coherence \u001b[38;5;129;01min\u001b[39;00m BOOLEAN_DOCUMENT_BASED \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.coherence == \u001b[33m'\u001b[39m\u001b[33mc_w2v\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Colin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\coherencemodel.py:546\u001b[39m, in \u001b[36mCoherenceModel.estimate_probabilities\u001b[39m\u001b[34m(self, segmented_topics)\u001b[39m\n\u001b[32m    543\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.coherence == \u001b[33m'\u001b[39m\u001b[33mc_w2v\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    544\u001b[39m         kwargs[\u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mself\u001b[39m.keyed_vectors\n\u001b[32m--> \u001b[39m\u001b[32m546\u001b[39m     \u001b[38;5;28mself\u001b[39m._accumulator = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmeasure\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprob\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._accumulator\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Colin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\topic_coherence\\probability_estimation.py:156\u001b[39m, in \u001b[36mp_boolean_sliding_window\u001b[39m\u001b[34m(texts, segmented_topics, dictionary, window_size, processes)\u001b[39m\n\u001b[32m    154\u001b[39m     accumulator = ParallelWordOccurrenceAccumulator(processes, top_ids, dictionary)\n\u001b[32m    155\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33musing \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m to estimate probabilities from sliding windows\u001b[39m\u001b[33m\"\u001b[39m, accumulator)\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43maccumulator\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccumulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Colin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\topic_coherence\\text_analysis.py:437\u001b[39m, in \u001b[36mParallelWordOccurrenceAccumulator.accumulate\u001b[39m\u001b[34m(self, texts, window_size)\u001b[39m\n\u001b[32m    436\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34maccumulate\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts, window_size):\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m     workers, input_q, output_q = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstart_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    438\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    439\u001b[39m         \u001b[38;5;28mself\u001b[39m.queue_all_texts(input_q, texts, window_size)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Colin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\topic_coherence\\text_analysis.py:471\u001b[39m, in \u001b[36mParallelWordOccurrenceAccumulator.start_workers\u001b[39m\u001b[34m(self, window_size)\u001b[39m\n\u001b[32m    469\u001b[39m     accumulator = PatchedWordOccurrenceAccumulator(\u001b[38;5;28mself\u001b[39m.relevant_ids, \u001b[38;5;28mself\u001b[39m.dictionary)\n\u001b[32m    470\u001b[39m     worker = AccumulatingWorker(input_q, output_q, accumulator, window_size)\n\u001b[32m--> \u001b[39m\u001b[32m471\u001b[39m     \u001b[43mworker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    472\u001b[39m     workers.append(worker)\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m workers, input_q, output_q\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Colin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\process.py:121\u001b[39m, in \u001b[36mBaseProcess.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process._config.get(\u001b[33m'\u001b[39m\u001b[33mdaemon\u001b[39m\u001b[33m'\u001b[39m), \\\n\u001b[32m    119\u001b[39m        \u001b[33m'\u001b[39m\u001b[33mdaemonic processes are not allowed to have children\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    120\u001b[39m _cleanup()\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28mself\u001b[39m._popen = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[38;5;28mself\u001b[39m._sentinel = \u001b[38;5;28mself\u001b[39m._popen.sentinel\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Colin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\context.py:224\u001b[39m, in \u001b[36mProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    222\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mProcess\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Colin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\context.py:336\u001b[39m, in \u001b[36mSpawnProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    333\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m    335\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpopen_spawn_win32\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Colin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\popen_spawn_win32.py:94\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     93\u001b[39m     reduction.dump(prep_data, to_child)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     \u001b[43mreduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     96\u001b[39m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Colin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\reduction.py:60\u001b[39m, in \u001b[36mdump\u001b[39m\u001b[34m(obj, file, protocol)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdump\u001b[39m(obj, file, protocol=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     59\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# === LDA K micro-sweep (K = 2..6) â€” reuse stratified split, CSV, and plots ===\n",
    "import os, math, json, random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# ---- Config ----\n",
    "OUT_DIR   = r\"C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\final\"\n",
    "INPUT     = os.path.join(OUT_DIR, \"Filtered_Combined_SD_Cleaned.parquet\")\n",
    "DICT_PATH = os.path.join(OUT_DIR, \"lda_dictionary_SD.dict\")\n",
    "\n",
    "K_GRID = list(range(2, 7))  # 2..6 inclusive\n",
    "RANDOM_STATE = 11\n",
    "PASSES, ITERS = 5, 400       # can bump after picking K\n",
    "CHUNKSIZE = 2000\n",
    "WORKERS = os.cpu_count()\n",
    "\n",
    "RESULTS_CSV   = os.path.join(OUT_DIR, \"lda_k_selection_SD_metrics_2_6.csv\")\n",
    "SPLIT_JSON    = os.path.join(OUT_DIR, \"lda_split_SD_stratified.json\")\n",
    "PLOT_COMBINED = os.path.join(OUT_DIR, \"lda_k_sweep_SD_2_6.png\")\n",
    "PLOT_COH_ONLY = os.path.join(OUT_DIR, \"lda_k_sweep_SD_2_6_coherence.png\")\n",
    "PLOT_LP_ONLY  = os.path.join(OUT_DIR, \"lda_k_sweep_SD_2_6_logperp.png\")\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ---- Load data/dictionary ----\n",
    "print(\"ðŸ“‚ Loading data/dictionary...\")\n",
    "df = pd.read_parquet(INPUT)\n",
    "texts = df[\"tokens\"].tolist()\n",
    "games = df[\"game\"].tolist() if \"game\" in df.columns else [\"ALL\"] * len(texts)\n",
    "dictionary = Dictionary.load(DICT_PATH)\n",
    "corpus = [dictionary.doc2bow(t) for t in texts]\n",
    "print(f\"âœ… docs={len(corpus)}  vocab={len(dictionary)}\")\n",
    "\n",
    "# ---- Load or create stratified split (90/10 by game) ----\n",
    "if os.path.exists(SPLIT_JSON):\n",
    "    print(f\"ðŸ” Using existing split: {SPLIT_JSON}\")\n",
    "    with open(SPLIT_JSON, \"r\") as f:\n",
    "        split = json.load(f)\n",
    "    train_idx, test_idx = split[\"train_idx\"], split[\"test_idx\"]\n",
    "else:\n",
    "    print(\"ðŸ†• Creating stratified split (90/10 by game)...\")\n",
    "    rng = random.Random(RANDOM_STATE)\n",
    "    by_game = defaultdict(list)\n",
    "    for i, g in enumerate(games): by_game[g].append(i)\n",
    "\n",
    "    hold_idx = set()\n",
    "    for g, idxs in by_game.items():\n",
    "        rng.shuffle(idxs)\n",
    "        k = max(1, int(0.10 * len(idxs)))\n",
    "        hold_idx.update(idxs[:k])\n",
    "\n",
    "    train_idx = [i for i in range(len(corpus)) if i not in hold_idx]\n",
    "    test_idx  = [i for i in range(len(corpus)) if i in hold_idx]\n",
    "    with open(SPLIT_JSON, \"w\") as f:\n",
    "        json.dump({\"random_state\": RANDOM_STATE, \"train_idx\": train_idx, \"test_idx\": test_idx}, f)\n",
    "\n",
    "train_corpus = [corpus[i] for i in train_idx]\n",
    "test_corpus  = [corpus[i] for i in test_idx]\n",
    "train_texts  = [texts[i] for i in train_idx]\n",
    "print(f\"ðŸ§ª Train: {len(train_corpus)}  Test: {len(test_corpus)}\")\n",
    "\n",
    "# ---- Train/eval helper ----\n",
    "def train_eval(k: int):\n",
    "    model = LdaMulticore(\n",
    "        corpus=train_corpus,\n",
    "        id2word=dictionary,\n",
    "        num_topics=k,\n",
    "        passes=PASSES,\n",
    "        iterations=ITERS,\n",
    "        random_state=RANDOM_STATE,\n",
    "        workers=WORKERS,\n",
    "        chunksize=CHUNKSIZE,\n",
    "        eval_every=None,\n",
    "        # alpha='asymmetric', eta=None  # optional to try later\n",
    "    )\n",
    "    c_v = CoherenceModel(model=model, texts=train_texts, dictionary=dictionary, coherence=\"c_v\").get_coherence()\n",
    "    log_perp = model.log_perplexity(test_corpus)  # higher (less negative) is better\n",
    "    return model, c_v, log_perp\n",
    "\n",
    "# ---- Sweep ----\n",
    "rows = []\n",
    "best = {\"k\": None, \"c_v\": -math.inf, \"log_perplexity\": -math.inf, \"model\": None}\n",
    "\n",
    "for k in K_GRID:\n",
    "    print(f\"\\nâ³ Training LDA (k={k}) ...\")\n",
    "    model, c_v, log_perp = train_eval(k)\n",
    "    print(f\"ðŸ“ˆ k={k} | c_v={c_v:.4f} | log_perplexity={log_perp:.4f} (higher = better)\")\n",
    "    rows.append({\"k\": k, \"c_v\": c_v, \"log_perplexity\": log_perp})\n",
    "    if (c_v > best[\"c_v\"]) or (math.isclose(c_v, best[\"c_v\"], rel_tol=1e-6) and log_perp > best[\"log_perplexity\"]):\n",
    "        best.update({\"k\": k, \"c_v\": c_v, \"log_perplexity\": log_perp, \"model\": model})\n",
    "\n",
    "# ---- Save metrics ----\n",
    "dfm = pd.DataFrame(rows).sort_values(\"k\")\n",
    "dfm.to_csv(RESULTS_CSV, index=False)\n",
    "print(f\"\\nðŸ“ Saved metrics -> {RESULTS_CSV}\")\n",
    "print(f\"ðŸ† Best K={best['k']} | c_v={best['c_v']:.4f} | log_perplexity={best['log_perplexity']:.4f}\")\n",
    "\n",
    "# ---- Plots ----\n",
    "def plot_combined(df, best_k, out_path):\n",
    "    df = df.sort_values(\"k\")\n",
    "    fig, ax1 = plt.subplots(figsize=(9, 5))\n",
    "    ax1.plot(df[\"k\"], df[\"c_v\"], marker=\"o\", label=\"c_v\")\n",
    "    ax1.set_xlabel(\"K (number of topics)\")\n",
    "    ax1.set_ylabel(\"Coherence (c_v)\")\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(df[\"k\"], df[\"log_perplexity\"], marker=\"s\", linestyle=\"--\", label=\"log_perplexity\")\n",
    "    ax2.set_ylabel(\"log_perplexity (higher is better)\")\n",
    "\n",
    "    ax1.axvline(best_k, linestyle=\":\", linewidth=1.5)\n",
    "    ax1.set_title(f\"LDA K Sweep (K=2..6) â€” Best K={best_k}\")\n",
    "\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc=\"best\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"ðŸ–¼ï¸ Saved combined plot -> {out_path}\")\n",
    "\n",
    "def plot_single(x, y, ylabel, title, out_path, marker=\"o\"):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(x, y, marker=marker)\n",
    "    plt.xlabel(\"K (number of topics)\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"ðŸ–¼ï¸ Saved plot -> {out_path}\")\n",
    "\n",
    "plot_combined(dfm, best[\"k\"], PLOT_COMBINED)\n",
    "plot_single(dfm[\"k\"], dfm[\"c_v\"], \"Coherence (c_v)\", \"LDA K Sweep â€” Coherence (2..6)\", PLOT_COH_ONLY, marker=\"o\")\n",
    "plot_single(dfm[\"k\"], dfm[\"log_perplexity\"], \"log_perplexity (higher is better)\", \"LDA K Sweep â€” log_perplexity (2..6)\", PLOT_LP_ONLY, marker=\"s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Loading full corpus/dictionary...\n",
      "âœ… docs=200993  vocab=27106\n",
      "â³ Training FINAL model on ALL docs (K=3) ...\n",
      "ðŸ’¾ Saved final model -> C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\final\\final_lda_SD_k3.model\n",
      "ðŸ—‚ï¸ Topic words saved -> C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\final\\final_topics_SD_k3.csv\n",
      "ðŸ“Š Per-game prevalence saved -> C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\final\\final_topic_by_game_SD_k3.csv\n",
      "ðŸ§¾ Per-doc dominant topics saved -> C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\final\\final_doc_topics_SD_k3.csv\n",
      "âœ… Finalization complete.\n"
     ]
    }
   ],
   "source": [
    "# === Finalize: Train the final K=3 model on the FULL corpus and export artifacts ===\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "OUT_DIR   = r\"C:\\Users\\colin\\Box\\2024-colin-viktor\\Videogame Scraping Project\\data\\final\"\n",
    "INPUT     = os.path.join(OUT_DIR, \"Filtered_Combined_SD_Cleaned.parquet\")\n",
    "DICT_PATH = os.path.join(OUT_DIR, \"lda_dictionary_SD.dict\")\n",
    "\n",
    "BEST_K    = 3\n",
    "RND       = 11\n",
    "PASSES    = 20\n",
    "ITERS     = 1000\n",
    "CHUNKSIZE = 2000\n",
    "WORKERS   = os.cpu_count()\n",
    "\n",
    "print(\"ðŸ“‚ Loading full corpus/dictionary...\")\n",
    "df = pd.read_parquet(INPUT)\n",
    "texts = df[\"tokens\"].tolist()\n",
    "games = df[\"game\"].tolist() if \"game\" in df.columns else [\"ALL\"] * len(texts)\n",
    "dictionary = Dictionary.load(DICT_PATH)\n",
    "corpus = [dictionary.doc2bow(t) for t in texts]\n",
    "print(f\"âœ… docs={len(corpus)}  vocab={len(dictionary)}\")\n",
    "\n",
    "print(f\"â³ Training FINAL model on ALL docs (K={BEST_K}) ...\")\n",
    "final_model = LdaMulticore(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=BEST_K,\n",
    "    passes=PASSES,\n",
    "    iterations=ITERS,\n",
    "    random_state=RND,\n",
    "    workers=WORKERS,\n",
    "    chunksize=CHUNKSIZE,\n",
    "    eval_every=None,\n",
    "    alpha='asymmetric',\n",
    "    eta='auto'\n",
    ")\n",
    "\n",
    "# ---- Save model ----\n",
    "final_model_path = os.path.join(OUT_DIR, f\"final_lda_SD_k{BEST_K}.model\")\n",
    "final_model.save(final_model_path)\n",
    "print(f\"ðŸ’¾ Saved final model -> {final_model_path}\")\n",
    "\n",
    "# ---- Export: topic top terms ----\n",
    "rows = []\n",
    "for t in range(BEST_K):\n",
    "    for rank, (w, p) in enumerate(final_model.show_topic(t, topn=25), start=1):\n",
    "        rows.append({\"topic\": t, \"rank\": rank, \"word\": w, \"prob\": p})\n",
    "topics_csv = os.path.join(OUT_DIR, f\"final_topics_SD_k{BEST_K}.csv\")\n",
    "pd.DataFrame(rows).to_csv(topics_csv, index=False)\n",
    "print(f\"ðŸ—‚ï¸ Topic words saved -> {topics_csv}\")\n",
    "\n",
    "# ---- Export: per-game topic prevalence ----\n",
    "# Assign each doc to its most probable topic\n",
    "doc_top = final_model.get_document_topics\n",
    "topic_game_counts = defaultdict(lambda: Counter())\n",
    "for i, bow in enumerate(corpus):\n",
    "    dt = doc_top(bow)\n",
    "    if not dt:\n",
    "        continue\n",
    "    top_topic, top_prob = max(dt, key=lambda x: x[1])\n",
    "    topic_game_counts[top_topic][games[i]] += 1\n",
    "\n",
    "pg_rows = []\n",
    "for t in range(BEST_K):\n",
    "    total = sum(topic_game_counts[t].values())\n",
    "    for g, c in topic_game_counts[t].items():\n",
    "        pg_rows.append({\n",
    "            \"topic\": t,\n",
    "            \"game\": g,\n",
    "            \"count\": c,\n",
    "            \"share_in_topic\": (c / total) if total else 0.0\n",
    "        })\n",
    "per_game_csv = os.path.join(OUT_DIR, f\"final_topic_by_game_SD_k{BEST_K}.csv\")\n",
    "pd.DataFrame(pg_rows).to_csv(per_game_csv, index=False)\n",
    "print(f\"ðŸ“Š Per-game prevalence saved -> {per_game_csv}\")\n",
    "\n",
    "# ---- Export: per-doc dominant topic (optional but handy) ----\n",
    "doc_rows = []\n",
    "for i, bow in enumerate(corpus):\n",
    "    dt = doc_top(bow)\n",
    "    if dt:\n",
    "        t, p = max(dt, key=lambda x: x[1])\n",
    "    else:\n",
    "        t, p = -1, 0.0\n",
    "    doc_rows.append({\n",
    "        \"doc_id\": i,\n",
    "        \"game\": games[i],\n",
    "        \"dominant_topic\": t,\n",
    "        \"dominant_prob\": p\n",
    "    })\n",
    "per_doc_csv = os.path.join(OUT_DIR, f\"final_doc_topics_SD_k{BEST_K}.csv\")\n",
    "pd.DataFrame(doc_rows).to_csv(per_doc_csv, index=False)\n",
    "print(f\"ðŸ§¾ Per-doc dominant topics saved -> {per_doc_csv}\")\n",
    "\n",
    "print(\"âœ… Finalization complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
